\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{color}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[hyphens]{url}
\usepackage{pgf, tikz}
\usetikzlibrary{arrows, automata}
\usepackage{titling}
\usepackage{varwidth}
\usepackage{hyperref}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}
\setlength\parindent{0pt}
\usepackage{float}


\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\usepackage{geometry}
 \geometry{
 a4paper,
 total={165mm,257mm},
 left=20mm,
 top=20mm,
 }

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}


\title{Statistical Machine Learning 2018\\Assignment 1\\Deadline: 7th of October 2018}
\author{
  Christoph Schmidl\\ s4226887\\      \texttt{c.schmidl@student.ru.nl}
  \and
  Mark Beijer\\ s4354834\\     \texttt{mbeijer@science.ru.nl}
}
\date{\today}

\begin{document}
\maketitle


\section*{Exercise 1 - weight 5}

Consider once more the M-th order polynomial

\begin{equation}
	y(x;w) = w_0 + w_1x + ... + w_Mx^M = \sum_{j = 0}^M w_jx^j
\end{equation}

\subsection*{1.1}

Create the function $f(x) = 1 + sin(6(x-2))$ in MATLAB. Generate a data set $\mathcal{D}_{10}$ of 10 noisy observation of this function. Take the 10 inputs spaced uniformly in range $[0,1]$, and assume that the noise is gaussian with mean 0 and standard deviation 0.3. $\mathcal{D}_{10}$ will be the training set. In a similar way, generate an additional test set $\mathcal{T}$ of 100 noisy observations over the same interval. Plot both the function and observations in $\mathcal{D}_{10}$ in a single graph (similar to Bishop, Fig.1.2).\\

\textbf{Answer:}\\

Throughout this assignment we are going to use Python for all our implementations and make heavy use of the library "numpy". The following code listing generated figure \ref{fig:1.1}.

\begin{lstlisting}[language=Python]
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

# numpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)
# Return evenly spaced numbers over a specified interval.

# Exercise 1.1

def f(x):
	return 1 + np.sin(6*(x-2))

def noisy_function(func, func_argument):
	noise = np.random.normal(0, 0.3)
	return noise + func(func_argument)

def generate_data(amount_of_datapoints):
    return [noisy_function(f, x) for x in np.linspace(0, 1, amount_of_datapoints)]

def plot_data(dataset):
    # plot the dataset
    plt.scatter(np.linspace(0, 1, 10), dataset, label='noisy observations')
    # plot the actual function
    X = np.linspace(0, 1, 100) # the higher the num value, the smoother the function plot gets
    y = [f(x) for x in X]
    plt.plot(X, y, color='green', label='True function')
    # plt.xlim(xmin=0)
    plt.ylabel('t')
    plt.xlabel('x')
    plt.legend(loc='upper right')
    # fancy caption. Not needed if latex is doing the job later on
    #txt="I need the caption to be present a little below X-axis"
    #plt.figtext(0.5, -0.05, txt, wrap=True, horizontalalignment='center', fontsize=12)
    plt.savefig('exercise_1_1.png')
    plt.show()

# Generating data set D_10 of 10 noisy observations
training_set = generate_data(10)

# Generating data set T of 100 noisy observations
test_set = generate_data(100)

# Plotting the function and observations in D_10
plot_data(training_set)
\end{lstlisting}


\begin{figure}[H]

\begin{center}
\includegraphics[width=0.75\textwidth]{Images/exercise_1_1.png}
\caption{Plot of noisy observations generated by the true underlying function incorporating gaussian noise.}
\label{fig:1.1}
\end{center}
\end{figure}


\subsection*{1.2}

Create a MATLAB function $w = PolCurFit(\mathcal{D}_N,M)$ that takes as input a data set $\mathcal{D}_N$, consisting of $N$ input/output-pairs $\{ x_n, t_n\}$, and a parameter $M$, representing the order of the polynomial in (1), and outputs a vector of weights $w = [w_0, ...,w_M]$ that minimizes the sum-of-squares error function

\begin{equation}
	E(w) = \frac{1}{2} \sum_{n=1}^N \{ y(x_n;w) - t_n\}^2
\end{equation}

Hint: use the results from the Tutorial Exercises (Week1, Exercise 5), and the \textbackslash-operator (backslash) in MATLAB to solve a linear system of equations.\\

\textbf{Answer:}\\

\begin{lstlisting}[language=Python]
def SSE(observations, targets):
    """ Calculate the sum-of-squares error. """
    return 0.5 * np.sum((observations - targets)**2)


def pol_cur_fit(data, polynomial_order):
    """ Return weights for an optimal polynomial curve fit. """
    
    observations = data[0, :] # Get me the first row, D_N
    targets = data[1, :] # Get me the second row, M
    
    # observation matrix
    A = np.zeros((polynomial_order, polynomial_order)) # Create matrix
    for i in range(polynomial_order):
        for j in range(polynomial_order):
            A[i, j] = np.sum(observations ** (i+j))
    
    # target vector        
    B = np.zeros(polynomial_order)
    for i in range(polynomial_order):
        B[i] = np.sum(targets * observations**i)
    
    # numpy.linalg.solve(a, b)
    # Solve a linear matrix equation, or system of linear scalar equations.
    # Computes the “exact” solution, x, of the well-determined, i.e., full rank, linear matrix equation ax = b.
    
    # Here's where the magic happens. Solve the linear system.
    weights = np.linalg.solve(A, B)
    return weights
\end{lstlisting}


\subsection*{1.3}

For the given dataset $\mathcal{D}_{10}$, run the $PolCurFit()$ function for $M = [0,...,9]$, and,

\begin{itemize}
	\item Plot for various orders $M$ (at least for $M = 0, M = 1, M = 3, M = 9$) the resulting polynomial, together with the function $f$ and observation $\mathcal{D}_{10}$ (similar to Bishop, Fig 1.4)
	\item For each order $M \in [0,...,9]$, compute the root-mean-square error
	
	\begin{equation}
		E_{RMS} = \sqrt{2E(w^*)/N}
	\end{equation}
	
	of the corresponding polynomial, evaluated on both the training set $\mathcal{D}_{10}$ and the test set $\mathcal{T}$. Plot both as a function of $M$ in a single graph. (see Bishop, Fig.1.5).\\
\end{itemize}

\textbf{Answer:}\\



\subsection*{1.4}


Repeat this procedure for a data set $\mathcal{D}_{40}$ of 40 observations (with the same noise level) and compare with the previous result.\\

\textbf{Answer:}\\


\subsection*{1.5}

Modify the $PolCurFit()$ function to include an additional penalty parameter $\lambda$, for a procedure that solves the minimization problem for a modified error function with quadratic regularizer (weight decay), given as 

\begin{equation}
	\widetilde{E} = E + \frac{\lambda}{2} \sum_{j = 0}^M w_j^2
\end{equation}

Verify that the regularizer drives the weights of high order terms in the polynomial to zero, and see if you can reproduce the explain the effect observed in Bishop, Fig.1.8.\\

\textbf{Answer:}\\


\subsection*{1.6}

The polynomial curve fitting procedure can be extened to the case of multidimensional inputs. Assuming an input vector of dimension $D$, namely $x = (x_1,x_2,...,x_D)$, we can write the regression function $y$ as:

\begin{equation}
	y(x;w) = \sum_{j=0}^M \Bigg( \sum_{n_1+n_2+...n_D=j} w_{n_1 n_2...n_D}x_1^{n_1}x_2^{n_2} ... x_D^{n_D}\Bigg)
\end{equation}

In the last expression, $j$ refers to the order of the polynomial terms. The inner sum is over all the combinations of non-negative integers $n_1, n_2, ..., n_D$, such that the constraint $n_1+ n_2 + ... + n_D = j$ holds. The terms $n_1, n_2,..., n_D$ correspond to the exponent for each variable $x_1,x_2,...,x_D$ in their respective polynomial term.\\

Note that if $D = 1$, the above expression simplifies to the formula in Equation (1). The reason the second sum disappears is that there is only one combination of the non-negative integer $n_1$ for which the constraint $n_1 = j$ holds, which means that there is only a single term to sum over.\\

Fitting the polynomial curve to a multidimensional input vector works analogously to the one-dimensional case. However, the number of parameters (the size of w) becomes much larger, even when $D = 2$. Write down the general polynomial curve equation in (5) for $D = 2$. How many parameters are needed in the two-dimensional case? Compare this to the number of parameters in the one-dimensional case.\\


\textbf{Answer:}\\

\section*{Exercise 2 - weight 2.5}

In this exercise, we consider the gradient descent algorithm for function minimization. When the function to be minimized $E(x)$, the gradient descent iteration is

\begin{align*}
	x_{n+1} = x_n - \eta \nabla E(x_n)
\end{align*}

where $\eta > 0$ is the so-called learning rate. In the following, we will apply gradient descent to the function

\begin{align*}
	h(x,y) = 100(y - x^2)^2 + (1 - x)^2
\end{align*}

\subsection*{2.1}

Make a plot of the function $h$ over the interval $[-2 \leq x \leq 2] \times [-1 \leq y \leq 3]$. Tip: use MATLAB function \textbf{surf}. Can you guess from the plot if numerical minimization with gradient descent will be fast or slow for this function?\\

\textbf{Answer:}\\

I made the function using the following code:


\begin{lstlisting}[language=Python]
def H(x,y):
    return 100*(y-x**2)**2+(1-x)**2
\end{lstlisting}

This returns h as defined by (7) in the assignment.

To create the data I used the following code:

\begin{lstlisting}[language=Python]
Amount = 1000
xmin = -2
xmax = 2
ymin = -1
ymax = 3
x = np.linspace(xmin,xmax,Amount)
y = np.linspace(ymin,ymax,Amount)
x,y = np.meshgrid(x,y)
z = H(x,y)
\end{lstlisting}

This first creates a row of 1000 numbers, with the boundaries as given in the assignment. Then the meshgrid function a 2d array so every combination $(x_i,y_i)$ in these boundaries is found at some (x[i,j],y[i,j]). Then we calculate z using our function.

For plotting I used the following code:

\begin{lstlisting}[language=Python]
fig = plt.figure()
ax = fig.gca(projection='3d')
surf = ax.plot_surface(x, y, z, cmap=cm.coolwarm,
                       linewidth=0, antialiased=False)
fig.colorbar(surf, shrink=0.5, aspect=5)
\end{lstlisting}


This creates a figure and axes to plot on. Then I used a surface plot which takes the 2d x,y,z values and plots them. I used a colormap to also show the value of z using colors. The last line plots this colorbar.

It looks like the function is quite steep and so the numerical minimization with gradient descent might be fast. For if it's steep it will make a big jump towards the minimum with the gradient descent rule.

\subsection*{2.2}

Knowing that a critical point of a function is a point where the gradient vanishes, show that $(1,1)$ is the unique critical point og $h$. Prove that this point is a minimum for $h$.\\

\textbf{Answer:}\\

The gradient of h is given by:

\begin{equation}
\nabla h(x,y) = \begin{bmatrix}
200(y-x^2)\cdot -2x -(1-x)\cdot 2\\
200(y-x^2)
\end{bmatrix} = \begin{bmatrix}
400x^3-400xy+2x-2\\
200y-200x^2
\end{bmatrix}
\end{equation}

Now if we fill in the point (1,1):

\begin{equation}
\nabla h(1,1) = \begin{bmatrix}
400-400+2-2\\
200-200
\end{bmatrix} = \begin{bmatrix}
0\\
0
\end{bmatrix}
\end{equation}

So this is a critical point of the function. To show it's the one and only critical point I will substitute y in the first equation by the second.

These two equations need to equal 0:

\begin{eqnarray}
400x^3-400xy+2x-2 &=& 0\\
200y-200x^2 &=& 0
\end{eqnarray}

So to solve it:

\begin{eqnarray}
y = x^2\\
400x^3 -400x^3 + 2x - 2 &=& 0\\
2x-2 &=& 0\\
2x &=& 2\\
x &=& 1\\
y &=& 1
\end{eqnarray}

And as we see, the only solution we find is the solution (1,1).

To prove it's the minimum we calculate the Hessian of h:

\begin{eqnarray}
\bold H(x,y) &=& \begin{pmatrix}
1200x^2 - 400y + 2 & -400x\\
-400x & 200
\end{pmatrix}
\end{eqnarray}

Which at the point of interest is:

\begin{equation}
\bold H(1,1) = \begin{bmatrix}
798 & -400\\
-400 & 200
\end{bmatrix}
\end{equation}

Here we see that $H_{xx} > 0$ and $H_{yy} > 0$, for $H_{ij} = \frac{\partial^2 H}{\partial i \partial j}$. This proves the point is a minimum.

\subsection*{2.3}

Write down the gradient descent iteration rule for this function.

\textbf{Answer:}\\


\begin{equation}
\vec x_{n+1} = \vec x_n - \eta \begin{bmatrix}
400x^3-400xy+2x-2\\
200y-200x^2
\end{bmatrix}(\vec x_n)
\end{equation}

\subsection*{2.4}

Implement gradient descent in MATLAB. Try some different values of $\eta$. Does the algorithm converge? How fast? Make plots of the trajectories on top of a contour plot of $h$. (Hint: have look at the MATLAB example code $contour\_example.m$ on Brightspace for inspiration to plot contours of functions and trajectories). Report your findings. Explan why numerical minimization with gradient descent is slow for this function.\\

\textbf{Answer:}\\

The problem with this is that the gradient is often quite high and the boundaries, but not as high in the neighbourhood of (1,1). So what often happens if the learning rate is too high that the next point makes a sudden step to some high value (say ($10^10$,5)), then it jumps to an even higher value, before jumping to such a high value that my computer can't calculate it. And even if that wasn't a problem it would constantly overshoot the (1,1). 



\newpage

One example of a trajectory is the following:

\begin{figure}[H]
\hspace*{-6cm}
\includegraphics[width=1.5\textwidth]{Images/Path_over_3d_surface.png}
\caption{Path over 3d surface. The orange indicates the path, the red the minimum (1,1). }
\end{figure}

A plot of how many times the walk converges you can see in the following figure. There we generated some random points and tracked how many of them converged in a certain amount of steps, given a certain $\eta$.
We chose the logarithm for $\eta$ so it can vary widely so we can find an optimal $\eta$.

\begin{figure}[H]
\hspace*{-6cm}
\includegraphics[width=1.5\textwidth]{Images/3dError.png}
\caption{How many random paths converge given the amount of steps and the logarithm of $\eta$.}
\end{figure}

The best results we've had was with  $\eta \approx 0.002$.


\section*{Exercise 3 - weight 2.5}


Suppose we have two healthy but curiously mixed boxes of fruit, with one box containing 8 apples and 4 grapefruits and the other containing 15 apples and 3 grapefruits. One of the boxes is selected at random and a piece of fruit is picked (but not eaten) from the chosen box, with equal probability for each item in the box. The piece of fruit is returned and then once again from the \textit{same} box a second piece is chosen at random. This is known as sampling with replacement. Model the box by random variable $\boldsymbol{B}$, the first piece of fruit by variable $\boldsymbol{F_1}$, and the second piece by $\boldsymbol{F_2}$.


\subsection*{3.1}

\textbf{Question:} What is the probability that the first piece of fruit is an apple given that the second piece of fruit was a grapefruit? How can the result of the second pick affect the probability of the first pick?\\

\textbf{Answer:}\\

Let's define the two random variables more specifically to later on save some writing. 

\begin{align*}
	B &= \{ 1,2\}\\
	F &= \{ A,G \}\\
	F_1 &= \texttt{First piece of fruit}\\
	F_2 &= \texttt{Second piece of fruit}
\end{align*}

The random variable $B$ (for Box) can take the values $1$ or $2$ for Box 1 or Box 2 respectively.\\
The random variable $F$ (for fruit) can take the values $A$ or $G$ for Apple or Grapefruit respectively.\\ 

Box 1 contains:

\begin{itemize}
	\item 8 apples
	\item 4 grapefruits
	\item 12 fruits in total
\end{itemize}

Box 2 contains :

\begin{itemize}
	\item 15 apples
	\item 3 grapefruits
	\item 18 fruits in total
\end{itemize}

To answer the question we have to find the following probability:

\begin{align*}
	p(F_1 = A | F_2 = G)\\	
\end{align*}



The tricky part is due to the fact that the piece of fruit is returned and then once again from the \textit{same} box a second piece is chosen at random. Otherwise we could just apply Bayes' theorem and would be done. Therefore we have to calculate the probability to pick a grapefruit or apple from each box separately.

\begin{align*}
	p(F = A | B = 1) &= \frac{2}{3}\\
	p(F = A | B = 2) &= \frac{5}{6}\\
	p(F = G | B = 1) &= 1 - p(F = A | B = 1) = 1 - \frac{2}{3} = \frac{1}{3}\\
	p(F = G | B = 2) &= 1 - p(F = G | B = 2) = 1 - \frac{5}{6} = \frac{1}{6}\\
	p(F = G) &= \frac{1}{3} \times \frac{1}{2} + \frac{1}{6} \times \frac{1}{2} = \frac{1}{4}
\end{align*}

\begin{align*}
	p(B = 1 | F = G) = \frac{p(F = G | B = 1)p(B = 1)}{p(F = G)} = \frac{\frac{1}{3} \times \frac{1}{2}}{\frac{1}{4}} = \frac{2}{3}\\
	p(B = 2 | F = G) = \frac{p(F = G | B = 2)p(B = 2)}{p(F = G)} = \frac{\frac{1}{6} \times \frac{1}{2}}{\frac{1}{4}} = \frac{1}{3}
\end{align*}

Now we can calculate the probability of each box:

\begin{align*}
	p(F_1 = A | F_2 = G) &= p(B = 1| F = G)p(F = A | B = 1) + p(B = 2 | F = G)p(F = A | B = 2)\\
	&= \frac{2}{3} \times \frac{2}{3} +  \frac{1}{3} \times \frac{5}{6}	\\
	&= \frac{4}{9} \times \frac{5}{18}\\
	&= \frac{13}{18} \approx 0.722
\end{align*}

The probability that the first fruit is an apple given that the second fruit is a Grapefruit picked from the same box as the apple is $\frac{13}{18}$.\\
The probability that the first fruit is an apple given that the second fruit is a grapefruit and both are picked from Box 1 is $\frac{8}{18}$, whereas the probability that the first fruit is an apple given that the second fruit is a grapefruit and both are picked from Box 2 is $\frac{5}{18}$. Therefore the second pick affects the probability of the first pick in such a way that is gives us a clue about the box it was picked from.

\subsection*{3.2}


\textbf{Question:} Imagine now that after we remove a piece of fruit, it is not returned to the box. This is known as sampling without replacement. In this situation, recompute the probability that the first piece of fruit is an apple given that the second piece of fruit was a grapefruit. Explain the difference.\\

\textbf{Answer:}\\

We want to find the following probability based on sampling without replacement:

\begin{align*}
	P(F_1 = A | F_2 = G)
\end{align*}

We have to extend the original formula from the previous exercise:

\begin{align*}
	p(F_1 = A | F_2 = G) &= p(B = 1| F = G)p(F = A | B = 1) + p(B = 2 | F = G)p(F = A | B = 2)\\
\end{align*}

And add the case for sampling without replacement. Therefore, the order of the pick is now important and the amount of fruits in a box changes accordingly:

\begin{align*}
	p(F_1 = A | B = 1) &= \frac{2}{3}\\
	p(F_1 = A | B = 2) &= \frac{5}{6}\\
	p(F_2 = G | B = 1) &= \frac{4}{11}  \quad \texttt{Removed one apple from box 1}\\
	p(F_2 = G | B = 2) &= \frac{3}{17} \quad \texttt{Removed one apple from box 2}\\
		p(F_2 = G) &= \frac{4}{11} \times \frac{1}{2} + \frac{3}{17} \times \frac{1}{2} = \frac{101}{374}
\end{align*}

\begin{align*}
	p(B = 1 | F_2 = G) = \frac{p(F_2 = G | B = 1)p(B = 1)}{p(F_2 = G)} = \frac{\frac{4}{11} \times \frac{1}{2}}{\frac{101}{374}} = \frac{68}{101}\\
	p(B = 2 | F_2 = G) = \frac{p(F_2 = G | B = 2)p(B = 2)}{p(F_2 = G)} = \frac{\frac{3}{17} \times \frac{1}{2}}{\frac{101}{374}} = \frac{33}{101}
\end{align*}

\begin{align*}
	p(F_1 = A | F_2 = G) &=   p(B = 1| F_2 = G)p(F_1 = A | B = 1) + p(B = 2 | F_2 = G)p(F_1 = A | B = 2)\\
	&= \frac{68}{101} \times \frac{2}{3} + \frac{33}{101} \times \frac{5}{6}\\
	&= \frac{437}{606} \approx 0.72112
\end{align*}

The difference between the probabilities of exercise 3.1 and exercise 3.2 are so minimal that they are nearly the same or not really significant. In the previous exercise the probability of $P(F_1 = A | F_2 = G)$ was $\frac{13}{18}$ which is $\approx 0.72$. In this exercise the probability of $P(F_1 = A | F_2 = G)$ was $\frac{437}{606}$ which is $\approx 0.72112$. The main difference is based on the fact that the total amount of fruits in a box changes after the first pick from either 12 to 11 for Box 1 or from 18 to 17 for Box 2. Because the removed fruit in the first pick is already fixed to being an apple, the probabilities of picking a grapefruit from the reduced amount of total fruits is minimal.


\subsection*{3.3}

\textbf{Question:} Starting from the initial situation (i.e.,sampling with replacement), we add a dozen oranges to the first box and repeat the experiment. Show that now the outcome of the first pick has no impact on the probability that the second pick is a grapefruit. Are the two picks now dependent or independent? Explain your answer.\\

\textbf{Answer:}\\

If we add a dozen (12) oranges to Box 1 then the total distribution among the boxes looks as follows:

Box 1 contains:

\begin{itemize}
	\item 8 apples
	\item 4 grapefruits
	\item 12 oranges
	\item 24 fruits in total
\end{itemize}

Box 2 contains :

\begin{itemize}
	\item 15 apples
	\item 3 grapefruits
	\item 18 fruits in total
\end{itemize}

In order to show that the outcome of the first pick has no impact on the probability that the second pick is a grapefruit, We have to show that

\begin{align*}
	p(F_2 = G | F_1 = A) = p(F_2 = G | F_1 = G) = p(F_2 = G | F_1 = O)\\
\end{align*}

\begin{align*}
	p(F = A | B = 1) &= \frac{1}{3}\\
	p(F = A | B = 2) &= \frac{5}{6}\\
	p(F = G | B = 1) &= \frac{1}{6}\\
	p(F = G | B = 2) &= \frac{1}{6}\\
	p(F = O | B = 1) &=  \frac{1}{2}\\
	p(F = O | B = 2) &=  0\\
	p(F = G) &= \frac{1}{6} \times \frac{1}{2} + \frac{1}{6} \times \frac{1}{2} = \frac{1}{6}\\
	p(F = A) &= \frac{1}{3} \times  \frac{1}{2} + \frac{5}{6} \times \frac{1}{2} = \frac{7}{12}\\
	p(F = O) &= \frac{1}{2} \times \frac{1}{2} + 0 \times \frac{1}{2} =  \frac{1}{4}
\end{align*}

\begin{align*}
	p(B = 1 | F = G) = \frac{p(F = G | B = 1)p(B = 1)}{p(F = G)} = \frac{\frac{1}{6} \times \frac{1}{2}}{\frac{1}{6}} = \frac{1}{2}\\
	p(B = 2 | F = G) = \frac{p(F = G | B = 2)p(B = 2)}{p(F = G)} = \frac{\frac{1}{6} \times \frac{1}{2}}{\frac{1}{6}} = \frac{1}{2}
\end{align*}

Calculating $p(F_2 = G | F_1 = A)$:

\begin{align*}
	p(F_1 = A | F_2 = G) &= p(B = 1| F = G)p(F = A | B = 1) + p(B = 2 | F = G)p(F = A | B = 2)\\
	&= \frac{1}{2} \times \frac{1}{3} +  \frac{1}{2} \times \frac{5}{6}\\
	&= \frac{7}{12}
\end{align*}


\begin{align*}
	p(F_2 = G | F_1 = A) = p(F_2 = G | F_1 = G) = p(F_2 = G | F_1 = O)\\
\end{align*}


\begin{align*}
	p(F_2 = G | F_1 = A) &= \frac{p(F_1 = A | F_2 = G)p(F_2 = G)}{p(F_1 = A)}\\
	&= \frac{\frac{7}{12} \times \frac{1}{6}}{\frac{7}{12}}\\
	&= \frac{1}{6}
\end{align*}


Calculating $p(F_2 = G | F_1 = G)$:

\begin{align*}
	p(F_1 = G | F_2 = G) &=  p(B = 1| F = G)p(F = G | B = 1) + p(B = 2 | F = G)p(F = G | B = 2)\\
	&= \frac{1}{2} \times \frac{1}{6} + \frac{1}{2} \times \frac{1}{6}\\
	&= \frac{1}{6}
\end{align*}

\begin{align*}
	p(F_2 = G | F_1 = G) &= \frac{p(F_1 = G | F_2 = G)p(F_2 = G)}{p(F_1 = G)}\\
	&= \frac{\frac{1}{6} \times \frac{1}{6}}{\frac{1}{6}}\\
	&= \frac{1}{6}
\end{align*}


Calculating $p(F_2 = G | F_1 = O)$:

\begin{align*}
	p(F_1 = O | F_2 = G) &= p(B = 1| F = G)p(F = O | B = 1) + p(B = 2 | F = G)p(F = O | B = 2)\\
	&= \frac{1}{2} \times \frac{1}{2} +  \frac{1}{2} \times 0\\
	&= \frac{1}{4}
\end{align*}

\begin{align*}
	p(F_2 = G | F_1 = O) &= \frac{p(F_1 = O | F_2 = G)p(F_2 = G)}{p(F_1 = O)}\\
	&= \frac{\frac{1}{4} \times \frac{1}{6}}{\frac{1}{4}}\\
	&= \frac{1}{6}
\end{align*}

Therefore the statement holds that:

\begin{align*}
p(F_2 = G | F_1 = A) &= p(F_2 = G | F_1 = G) = p(F_2 = G | F_1 = O)\\
\frac{1}{6} &= \frac{1}{6} = \frac{1}{6}
\end{align*}

The two picks are independent because the pick of the first fruit does not affect the pick of the second fruit and the probability is equal amongst the picks.

\section*{Exercise 4 - Bonus (weight 1)}

Given a joint probability function over the random vector $X = (X_1, X_2, X_3, X_4)$ that factorizes as

\begin{align*}
	p(x_1,x_2,x_3,x_4) = p(x_1,x_4|x_2)p(x_2,x_3|x_1)
\end{align*}

show (using the sum and product rules for marginals and conditionals) that the following independence statements hold:

\subsection*{4.1}

\begin{align*}
	X_1 \independent X_2
\end{align*}


\textbf{Answer:}\\

In order to show that $X_1 \independent X_2$, we have to prove that $p(x_1, x_2) = p(x_1)p(x_2)$. Figure \ref{fig:directed_graph} gives a visual representation of the factorization as a directed graph. We can already see that the statement $X_1 \independent X_2$ does not hold.


\begin{figure}[h]
\centering
    \begin{tikzpicture}[
            > = stealth, % arrow head style
            shorten > = 1pt, % don't touch arrow head to node
            auto,
            node distance = 3cm, % distance between nodes
            semithick % line style
        ]

        \tikzstyle{every state}=[
            draw = black,
            thick,
            fill = white,
            minimum size = 4mm
        ]

        \node[state] (x_1) {$x_1$};
        \node[state] (x_2) [above right of=x_1] {$x_2$};
        \node[state] (x_3) [below right of=x_1] {$x_3$};
        \node[state] (x_4) [right of=x_1] {$x_4$};
        

        \path[->] (x_1) edge node {} (x_2);
        \path[->] (x_2) edge[bend right] node {} (x_1);
        \path[->] (x_1) edge node {} (x_3);
        \path[->] (x_2) edge node {} (x_4);
      

    \end{tikzpicture}
\caption{Factorization as directed graph} \label{fig:directed_graph}
\end{figure}

We can rewrite the factorization as follows:

\begin{align*}
	p(x_1,x_2,x_3,x_4) &= p(x_1,x_4|x_2)p(x_2,x_3|x_1)\\
	&= p(x_1|x_2)p(x_4|x_2)p(x_2|x_1)p(x_3|x_1)
\end{align*}

We can now marginalize $x_1$ and $x_2$ and apply the sum and product rule at the end to get the joint probabilities:

\begin{align*}
	p(x_1) = \sum_{x_2} p(x_1|x_2)p(x_2) = \sum_{x_2} p(x_1,x_2)\\
	p(x_2) = \sum_{x_1} p(x_2|x_1)p(x_1) = \sum_{x_1} p(x_2, x_1)
\end{align*}

Therefore:

\begin{align*}
	p(x_1)p(x_2) = \sum_{x_2} p(x_1,x_2) \sum_{x_1} p(x_2, x_1)\\
	p(x_1, x_2) \neq p(x_1)p(x_2)
\end{align*}

Based on this contradiction the statement $X_1 \independent X_2$ does \textbf{not} hold.

\subsection*{4.2}

\begin{align*}
	X_3 \independent X_4 | X_1,X_2
\end{align*}

\textbf{Answer:}\\

In order to show that $X_3 \independent X_4 | X_1,X_2$, we have to prove that $p(x_3, x_4 | x_1, x_2) = p(x_3 | x_1, x_2)p(x_4 | x_1, x_2)$. 

We already know from the previous exercise that 

\begin{align*}
	p(x_1)p(x_2) = p(x_1,x_2)p(x_2,x_1)
\end{align*}

and based on the symmetry property we know that:

\begin{align*}
	p(x_1,x_2) = p(x_2,x_1)
\end{align*}

We can therefore replace the joint probabilities with marginal probabilities which makes it easier to prove the conditional independence:

\begin{align*}
	p(x_1,x_2) = p(x_1)\\
	p(x_1,x_2) = p(x_2)
\end{align*}

We can rewrite the statements as follows when we replace $p(x_1,x_2)$ for $p(x_1)$:

\begin{align*}
p(x_3, x_4 | x_1, x_2) = p(x_3 | x_1, x_2)p(x_4 | x_1, x_2)\\
p(x_3,x_4|x_1) = p(x_3|x_1)p(x_4|x_1)\\
p(x_3|x_1)p(x_4|x_1) = p(x_3|x_1)p(x_4|x_1)
\end{align*}

Therefore the statement $X_3 \independent X_4 | X_1,X_2$ holds.

\end{document}
