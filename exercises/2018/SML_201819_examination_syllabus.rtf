{\rtf1\ansi\ansicpg1252\cocoartf1348\cocoasubrtf170
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\paperw11900\paperh16840\margl1440\margr1440\vieww13900\viewh12420\viewkind0
\deftab720
\pard\pardeftab720

\f0\fs24 \cf0 \expnd0\expndtw0\kerning0
The SML exam 2018/2019 will cover the following subjects:\
(final version, 17/12/2018)\
\
- Lecture slides (incl. kernel slides, and guest lecture on EM, see details below)\
- Assignments\
\
The 
\b \expnd0\expndtw0\kerning0
exam is open book
\b0 \expnd0\expndtw0\kerning0
: You are allowed to 
\b \expnd0\expndtw0\kerning0
use \'91Bishop'
\b0 \expnd0\expndtw0\kerning0
 (Pattern Recognition and Machine Learning) during the exam. In addition, you can use 
\b \expnd0\expndtw0\kerning0
ONE "cheat sheet"
\b0 \expnd0\expndtw0\kerning0
 (size A4 paper sheet) during the exam. On this sheet you may put any notes or reminders, e.g. relations like log(a b) = log(a)+ log(b), that you think will be useful for you during the exam. A simple (non programmable) pocket calculator is recommended. Other resources like (smart)phones or printouts of the exercise+answers.pdfs are 
\b \expnd0\expndtw0\kerning0
NOT
\b0 \expnd0\expndtw0\kerning0
 allowed.\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardeftab720\pardirnatural
\cf0 \kerning1\expnd0\expndtw0 =================\
Contents from Bishop book + slides:\
\
1 Introduction \
	1.1 Polynomial curve fitting\
	1.2 Probability theory\
	1.3 Model selection\
	1.4 Curse of dimensionality\
	1.5 Decision theory\
	1.6 Information theory \
\
2 Probability distributions\
	2.1 Binary variables\
	2.2 Multinomial variables\
	2.3 Gaussian distribution + appendix C \

\i 		(not 2.3.4-5)\

\i0 		2.3.6 posterior over mean 
\i (not over sigma)
\i0 \

\i 		(not 2.3.7-8)\

\i0 		2.3.9 (see also Ch.9 EM treatment later)\

\i 	NOT 2.4 Exponential family \

\i0 	2.5 Non-parametric methods \
	\
3 Linear models for regression\
	3.1 Linear basis function models 
\i (not 3.1.3, 3.1.5)
\i0 \
	
\i NOT 3.2 Bias Variance decomposition
\i0 \
	3.3 Bayesian linear regression \
	3.4 Bayesian Model comparison\
	
\i NOT 3.5 Evidence approximation 
\i0 \

\i 	NOT 3.6 Limitations of Fixed basis functions 
\i0 \
\
4 Linear models for classification\
	4.1 Discriminant functions 
\i (not 4.1.3, 4.1.5, 4.1.6). 
\i0 \
	4.1.7 Fisher discriminant function for two classes. \
	4.2 Probabilistic generative models 
\i (not 4.2.3-4)
\i0 \
	4.3 Probabilistic discriminative models 
\i (not 4.3.4-6)
\i0 \

\i 	
\i0 4.4 Laplace approximation and BIC 
\i \
	NOT 4.5 Bayesian Logistic regression \

\i0 \
\
5 Neural Networks \
	5.1 Feedforward Network functions\
	5.2 Network training\
	5.3 Error Backpropagation 
\i (not 5.3.2-4)
\i0 \

\i 	NOT 5.4 The Hessian matrix \
	NOT 5.5 Regularization in Neural Networks\
	NOT 5.6 Mixture Density Networks \
	NOT 5.7 Bayesian Neural Networks \

\i0 \
6 Kernel Methods\
	6.1 Dual representations\
	6.2 Constructing Kernels\
	6.3 Radial Basis Function networks\
	6.4 Gaussian processes 
\i (not 6.4.5-7)
\i0 \
\
9 Mixture Models and EM (guest lecture T.Heskes)\
	9.1 K-means clustering\
	9.2 Mixtures of Gaussians\
	9.3 An alternative view of EM\
\
\pard\pardeftab720
\cf0 \expnd0\expndtw0\kerning0
Specific topics:\
- gradient descent, incl. stochastic- and conjugate-  (see 5.2.4)\
- Newtons method (eqn 4.92, p.207)\
- Elementary calculus and linear algebra (see e.g. 695)\
- Lagrange multipliers (see appendix E)\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardeftab720\pardirnatural
\cf0 \kerning1\expnd0\expndtw0 \
=================\
\
\
\
}