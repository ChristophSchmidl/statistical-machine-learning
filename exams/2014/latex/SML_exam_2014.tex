\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{color}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{multicol}
%\usepackage[lofdepth,lotdepth]{subfig}  This gives errors when used together with "subcaption", which is needed to create subfigures.
\usepackage{graphicx}
\usepackage{listings}
\usepackage[hyphens]{url}
\usepackage[official]{eurosym}
\usepackage{enumerate}
\usepackage{pgf, tikz}
\usetikzlibrary{arrows, automata}
\usepackage{titling}
\usepackage{varwidth}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}
\setlength\parindent{0pt}
\usepackage{float}
\usepackage{subcaption}
\usepackage{polynom}


\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\usepackage{geometry}
 \geometry{
 a4paper,
 total={165mm,257mm},
 left=20mm,
 top=20mm,
 }

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}



\begin{document}

\title{Tentamen: Statistical Machine Learning (NB054E)\\ \vspace{1em}21 January 2015, 08:30-11:30 in HG00.068}
\date{}
\maketitle

\vspace{-5em}
\textit{Write your \textbf{name and student number at the top of each sheet}. On each page, indicate page number and total number of pages.\\
\textbf{Please, write clearly!} Make sure to properly motivate all answers, and do not
forget to include intermediate steps in your calculations: even if your final answer is
wrong, you may still gain some points in that way. You may refer to the Bishop book
for relevant equations, etc. One personal “cheat sheet” (a single A4 paper sheet) is
allowed.}\\

\textbf{Total possible points: 80}




\section*{Assignment 1 (18pt)}

A factory produces \textit{fliggrs} $X$. Seventy-five percent (75\%) of the fliggrs has quality $x = 1$ and the rest has quality $x = 0$. Unfortunately, the quality of fliggrs cannot be determined directly. (To
asses these probabilities, destructive methods have been applied).\\
There is a possibility to test fliggr quality with a new type of test $Y$ . A test result can be positive $(y = 1)$ or negative $(y = 0)$. Studies have shown that 40\% of fliggrs with quality $x = 1$ have a positive test result. However, 30\% of fliggrs with $x = 0$ also test positive.\\

\textbf{Question 1.1} (4pt) \textit{What are the following probabilities according to the above stated description?}

\begin{multicols}{2}
\begin{enumerate}[i]
    \item $P(x = 1)$
    \item $P(y = 1 | x = 1)$
    \item $P(y = 1 | x = 0)$
    \item $P(y = 1)$
\end{enumerate}
\end{multicols}


\textbf{Question 1.2} (6pt)  \textit{Compute, using Bayes’ rule, the probability of quality $x = 1$ if the test result is positive. Do the same for the case that the test result is negative.}\\


The test $Y$ is cheap, but still there are costs (per fliggr) involved. The question is whether the test is economically beneficial, i.e. whether its use increases the expected profit.

\begin{enumerate}
	\item If a fliggr of quality $x = 1$ is correctly classified, it yields a profit of \euro{}40.-
	\item If a fliggr is classified as $x = 0$, it yields a profit of \euro{}18.-, regardless of its true quality.
	\item If a fliggr with true quality $x = 0$ is wrongly classified as $x = 1$, it causes a loss of \euro{}60.-
\end{enumerate}

Without the test we could either adopt a policy of classifying every fliggr as $x = 1$ or as $x = 0$.\\

\textbf{Question 1.3} (4pt) \textit{What is the expected profit per fliggr under optimal classification, without implementation of the test?}\\


Using test $Y$ we could adopt a more refined policy (dependent on the outcome of the test) to maximize the expected profit per fliggr. There are now four different classification policies: if the test result is positive $(y = 1)$, we can either classify the fliggr as $x = 1$ or as $x = 0$, and if the test result is negative $(y = 0)$, we also can choose between classifying the fliggr as $x = 1$ or as $x = 0$.\\


\textbf{Question 1.4} (4pt) \textit{Compute the expected profit per fliggr, assuming optimal classification dependent on the outcome of the test. What is the maximum price of the test per fliggr, if the test
is to be economically beneficial?}\\

\section*{Assignment 2 (24pt)}

The distribution of the number of occurences in a fixed period of time in systems with a large number of possible events, each of which is relatively rare, is modelled by the \textit{Poisson} distribution.
Examples are the number of accidents in a week for a certain stretch of road, or the number of
photons per second received by a detector from a distant star. The probability of $k$ such events in a fixed period of time is given as

\begin{equation}
\label{eq:poisson}
	Pois(k | \lambda) = \frac{\lambda^k \exp(-\lambda)}{k!}
\end{equation}

with parameter $\lambda > 0, k \in \{ 0,1,2,... \}$ and $k!$ the factorial of $k$.\\

\textbf{Question 2.1} (4pt) \textit{Verify that the Poisson distribution (\ref{eq:poisson}) represents a proper probability distribution.\\Hint: use the fact that $\sum_{n=0}^\infty \frac{x^n}{n!} = e^x$. Hint 2: do not confuse the variable with the parameter!}\\


For a 1 km stretch of road the number of accidents per week has been recorded over a period of several months, resulting in a data set $\textbf{X} = \{ x_1,..., x_N \}$. We assume the data can be modelled
as independent samples from a Poisson distribution and want to obtain an estimate for $\lambda$\\

\textbf{Question 2.2} (6pt) \textit{Show that the log-likelihood of $\lambda$ for this data set is given by}\\

\begin{equation}
\label{eq:log-likelihood}
\ln p(\textbf{X} | \lambda) = -N \lambda + K \ln(\lambda) - \sum_{i = 1}^N \ln(x_i!) 
\end{equation}

with $K = \sum_{i = 1}^N x_i$.\\


\textbf{Question 2.3} (4pt) \textit{From (\ref{eq:log-likelihood}) Show that the maximum likelihood estimate $\lambda_{ML}$ is given by}\\

\begin{equation}
\label{eq:MLE}
	\lambda_{ML} = \frac{K}{N}
\end{equation}

\textbf{Extra Question} \textit{Is $\lambda_{ML}$ a biased or unbiased estimator? Why?}\\


The recorded number of accidents per week over a 2 month period was as follows:

\begin{align*}
	\textbf{X} = \{ 4, 1, 0, 5, 2, 3, 0, 1\}
\end{align*}

\textbf{Question 2.4} (2pt) \textit{Calculate $\lambda_{ML}$ for this data set.}\\

Background information based on observations for similar types of roads has resulted in the following \textit{Gamma} distribution as prior over $\lambda$ per kilometer of road:

\begin{equation}
\label{eq:gamma}
 Gam(\lambda | a, b) = \frac{1}{\Gamma(a)} b^a \lambda^{a-1} \exp(-b \lambda)
\end{equation}

with hyperparameters $a = 3$ and $b = 2$ (see 2.146 in Bishop). The Gamma distribution is the conjugate prior to the likelihood function for the parameter $\lambda$ in the Poisson distribution, meaning
that the posterior has the same functional form as the prior.\\

\textbf{Question 2.5} (4pt) \textit{Show that with the Gamma prior (\ref{eq:gamma}) and a dataset \textbf{X}, the posterior distribution of $\lambda$ takes the form}

\begin{equation}
\label{eq:posterior}
p(\lambda | \textbf{X}) = Gam(\lambda | a + K, b + N)
\end{equation}

\textit{Hint: In the derivation, you can ignore factors not involving $\lambda$}\\

\textbf{Question 2.6} (4pt) \textit{Use the posterior distribution (\ref{eq:posterior}), together with (B.26-29) to obtain}

\begin{itemize}
	\item \textit{the Bayesian maximum a posteriori estimate $\lambda_{ML}$, i.e., the mode of the posterior distribution
of $\lambda$ }
	\item \textit{the posterior expected value $\mathbb{E}[\lambda | \textbf{X}]$}
\end{itemize}

\textit{for the given stretch of road. Compare with the maximum likelihood estimate $\lambda_{ML}$; why are these
estimates lower than the maximum likelihood estimate?}



\section*{Assignment 3 (18pt)}

Consider a probability distribution $p(u, v, w) = p(u)p(v)p(w | u, v)$\\

\textbf{Question 3.1} (4pt) \textit{Show this implies that variable $u$ is independent of $v$. (If you think that this makes it easier, you may assume that all three variables are discrete).}\\


We now consider a multivariate Gaussian probability distribution $p(u, v, w) = p(u)p(v)p(w | u, v)$ defined in terms of conditional distributions as

\begin{align}
	p(u) &= \mathcal{N}(u | \alpha, \rho^2)\\
	p(v) &= \mathcal{N}(v | \beta, \sigma^2)\\
	p(w | u, v) &= \mathcal{N}(w | \gamma(u + 2v), \tau^2)
\end{align}

with $\alpha, \beta, \gamma, \rho, \sigma$ and $\tau$ constant model parameters.\\

We are looking for the marginal distribution $p(w)$. Unfortunately, the equations (2.113-2.117) only consider the relation between two variables. To deal with this, we view $u$ and $v$ as two partitioned
Gaussian components of a single multivariate variable $\textbf{x} = (u,v)^T$, and write the distribution over this new variable as $p(x) = \mathcal{N}(x | x_0, \Sigma)$\\

\textbf{Question 3.2} (6pt) \textit{Give an expression for the mean $x_0$ and covariance $\Sigma$ in $p(x)$ in terms of the model parameters.}\\


The joint distribution can now be written in the form $p(x, w) = p(x)p(w | x)$. For the conditional distribution we write $p(w | x ) = \mathcal{N}(w | Ax, L^{-1})$\\

\textbf{Question 3.3} (8pt) \textit{(a) Give an expression for $\textbf{A}$ and $\textbf{L}^{-1}$ in $p(w | x)$ in terms of the model paramters.\\(b) Use this to obtain an expression for the mean and variance of the marginal distribution $p(w) = \mathcal{N}(w | w_0, \sigma^2_w)$ in terms of the model parameters.}\\


\section*{Assignment 4 (20pt)}

In a recent survey under master students at the RU, we collected a data set of 400 records of 3 variables $\textbf{x} = \{ gender, IQ, haircolour \}$, in which each value is represented as an integer number. We would like to use the ‘kernel trick’ to analyse this data.\\

\textbf{Question 4.1} (4pt) \textit{For this data set, show that}

\begin{equation}
	k(x, y) = (x^T y)^2 + 4x^T y + 1
\end{equation}

\textit{is a valid kernel function. What are the dimensions of the corresponding kernel matrix and the number of implied features?}\\


As this data set is too ambituous to tackle by hand in an exam, we als have another, more modest data set of observations: $(x_1, t_1) = (-1, 0),$ and $(x_2, t_2) = (1, 1)$. We assume that there is some
underlying function $y_i = f(x_i)$, for which we have noisy observations $t_i = y_i + \epsilon_i$ governed by
independent Gaussian noise, with precision parameter $\beta = 2$:

\begin{equation}
	p(t_i | y_i) = \mathcal{N}(t_i | y_i, \beta^{-1})
\end{equation}

We want to know the value $t$ we can expect to observe at $x = 0$. We decide to use a Gaussian process (GP), with a standard Gaussian kernel defined as 

\begin{equation}
k(x, x') = \theta_0 \exp \left\{ -\frac{ \theta_1}{2} || x - x' ||^2 \right\}
\end{equation}

with $\theta_0 = 1, \theta_1 = \ln(2) \approx 0.6931$\\

\textbf{Question 4.2} (4pt) \textit{In this GP approach, the marginal distribution $p(t)$ over $t = (t_1, t_2)^T$ (before the actual observation $t_1 = 0, t_2 = 1$), conditioned on input values $x_1 = -1, x_2 = 1$, takes the form of a multivariate Gaussian. Compute the mean $\mu$  and covariance matrix  $\Sigma$ of this distribution.}\\


\textbf{Question 4.3} (6pt) \textit{On observing $t_1 = 0, t_2 = 1$, the resulting probability distribution for the observation at $x = 0$ is again Gaussian. Compute mean and covariance for this distribution.}\\
\textit{\\Hint: remember that}

\[
A^{-1} = 
\begin{pmatrix}
a & b\\
c & d
\end{pmatrix}^{-1}
=\frac{1}{\det (A)}
\begin{pmatrix}
d & -b\\
-c & a
\end{pmatrix}
\]


\textbf{Question 4.4} (4pt) \textit{Does the maximum of the expected mean pass through the data points $(-1, 0)$ and $(1,1)$? If so, explain why this is logical for the given data set; if not, explain what should be changed to make it so.}\\

\textbf{Question 4.5} (2pt) \textit{What happens to the predictive mean of our GP regression result with Gaussian kernel as $x \to \pm \infty$? How does this differ from the solution for the Bayesian linear regression model $y(x, w) = w_0 + w_1x$ (Bishop, §3.3) with a zero mean Gaussian weight prior?}\\

\end{document}
