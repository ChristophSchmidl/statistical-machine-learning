\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{color}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{multicol}
%\usepackage[lofdepth,lotdepth]{subfig}  This gives errors when used together with "subcaption", which is needed to create subfigures.
\usepackage{graphicx}
\usepackage{listings}
\usepackage[hyphens]{url}
\usepackage{pgf, tikz}
\usetikzlibrary{arrows, automata}
\usepackage{titling}
\usepackage{varwidth}
\usepackage{hyperref}
\usepackage{color} %red, green, blue, yellow, cyan, magenta, black, white
\definecolor{mygreen}{RGB}{28,172,0} % color values Red, Green, Blue
\definecolor{mylilas}{RGB}{170,55,241}
\setlength\parindent{0pt}
\usepackage{float}
\usepackage{subcaption}
\usepackage{polynom}
\usepackage{physics}
\newcommand\tab[1][1cm]{\hspace*{#1}}


\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\usepackage{geometry}
 \geometry{
 a4paper,
 total={165mm,257mm},
 left=20mm,
 top=20mm,
 }

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{Statistical Machine Learning 2018\\Assignment 4\\Deadline: 11th of January 2019}
\author{
  Christoph Schmidl\\ s4226887\\      \texttt{c.schmidl@student.ru.nl}
  \and
  Mark Beijer\\ s4354834\\     \texttt{mbeijer@science.ru.nl}
}
\date{\today}

\begin{document}
\maketitle


\section*{Exercise 1 - Logistic regression (weight 5)}

\textbf{Part 1 - The IRLS algorithm}\\

Many machine learning problems require minimizing / maximizing some function $f(x)$. For this, an alternative to the familiar gradient descent technique, is the so called Newton-Raphson iterative method:

\begin{eqnarray} \label{eq:1}
\textbf{x}^{(n+1)} = \textbf{x}^{(n)} - \textbf{H}^{-1} \nabla f(\textbf{x}^{(n)})
\end{eqnarray}

where \textbf{H} represents the Hessian matrix of second derivatives of $f(\textbf{x})$, see Bishop, ยง4.3.3.

\subsection*{1.1.1}

Derive an expression for the minimization / maximization of the function $f(x) = \sin(x)$, using the Newton-Raphson iterative optimization scheme (\ref{eq:1}), and verify (using Matlab, just up to, e.g., five iterations) how quickly it converges when starting from $x^{(0)} = 1$. What happens when you start from $x^{(0)} = -1$?\\
Hint: The Hessian of a 1-dimensional function $f(x)$ is just the second derivative $f''$. So, the Newton-Raphson iterative method reduces in 1-d to 

\begin{eqnarray}  \label{eq:2}
x^{(n+1)} = x^{(n)} - \frac{f'(x^{(n)})}{f''(x^{(n)})}
\end{eqnarray}


\textbf{Answer:}\\

Expression for the minimization / maximization of the function $f(x) = \sin(x)$:

\begin{align*}
x^{(x+1)} &= x^{(n)} - \frac{\sin'(x^{(n)})}{\sin''(x^{(n)})}\\
&=  x^{(n)} - \frac{\cos(x^{(n)})}{-\sin(x^{(n)})}\\
&=  x^{(n)} + \frac{\cos(x^{(n)})}{\sin(x^{(n)})}\\
\end{align*}

\begin{lstlisting}[language=Python]
import numpy as np
import sympy as sp

# Exercise 1.1.1

## See also: https://docs.sympy.org/latest/tutorial/calculus.html

def f(x):
    return np.sin(x)

def iterative_optimization(x):
    """ Using Newton-Raphson iterative optimization scheme """
    return x + (np.cos(x)/np.sin(x))

x_t = 1
n_iterations = 5

print("Starting the optimization process with x_0: {}".format(x_t))

for i in range(n_iterations):
    x_t = iterative_optimization(x_t)
    print("Step {}: {}".format(i, x_t))
    
# Starting the optimization process with x_0: 1
# Step 0: 1.6420926159343308
# Step 1: 1.5706752771612507
# Step 2: 1.5707963267954879
# Step 3: 1.5707963267948966
# Step 4: 1.5707963267948966

# Starting the optimization process with x_0: -1
# Step 0: -1.6420926159343308
# Step 1: -1.5706752771612507
# Step 2: -1.5707963267954879
# Step 3: -1.5707963267948966
# Step 4: -1.5707963267948966
\end{lstlisting}



If we take $x_0 = 1$, then the algorithm converges after 4 steps towards $1.5707963267948966$ which is an approximation of $\frac{\pi}{2}$.\\ 
If we take $x_0 = -1$, then the algorithm converges after 4 steps towards $-1.5707963267948966$ which is an approximation of $-\frac{\pi}{2}$.

\begin{figure}[H]
\center
\includegraphics[width=0.6\textwidth]{Images/sine_cosine.png}
\caption{Sine and Cosine}
\label{Fig:sine-cosine}
\end{figure}

When we take a look at figure \ref{Fig:sine-cosine} then we can see that the maximum and minimum of the sine function is indeed at $(x = \frac{\pi}{2}, f(x) = 1)$ and $(x = -\frac{\pi}{2}, f(x) = -1)$ with possible shiftings to the right and left based on the periodic nature of the sine function.\\

Note: Also see \url{https://en.wikipedia.org/wiki/Newton%27s_method#Applications}


\subsection*{1.1.2}


We want to apply this method to the logistic regression model for classification (see Bishop, ยง4.3.2):

\begin{equation}  \label{eq:3}
p(\mathcal{C}_1 | \phi ) = y(\phi) = \sigma(w^T \phi)
\end{equation}

For a data set $\{ \phi_n, t_n\}^N_{n=1}$, with $t_n \in \{ 0,1 \}$, using $y_n = p(\mathcal{C}_1 | \phi_n)$ the corresponding cross entropy error function to minimize is 

\begin{equation}  \label{eq:4}
E(\textbf{w}) = - \sum^N_{n=1} \{ t_n \ln y_n + (1 - t_n) \ln(1 - y_n)\}
\end{equation}

With one basis function $\phi$ and the dummy basis function 1, the feature vector in (\ref{eq:3}) becomes $\phi = [1, \phi]^T$. The weight vector including the bias term is then also two dimensional, $\textbf{w} = [w_0,w_1]^T$.
Expressions for the gradient $\nabla E(\textbf{w})$ and Hessian \textbf{H} in terms of the data set are given in Bishop, eq.4.96-98. As both are implicitly dependent on the weights \textbf{w}, they have to be recalculated after each step: hence this is known as the 'Iterative Reweighted Least Squares' algorithm.\\

Consider the following data set: $\{ \phi_1, t_1 \} = \{0.3, 1 \}, \{ \phi_2, t_2 \} = \{ 0.44, 0 \}, \{ \phi_3, t_3 \} = \{ 0.46, 1 \}$ and $\{ \phi_4, t_4 \} = \{ 0.6, 0 \}$, and initial weight vector $\textbf{w}^{(0)} = [1.0, 1.0]^T$.\\

Show using e.g. a Matlab implementation that for this situation the IRLS algorithm converges in a few iterations to the optimal solution $\hat{\textbf{w}}^T \approx [9.8, -21.7]$, and show that this solution corresponding to a decision boundary $\phi = 0.45$ in the logistic regression model. (The IRLS algorithm should take about five lines of Matlab code inside a loop + initialization).\\


\textbf{Answer:}\\

\begin{align*}
p(\mathcal{C}_1 | \phi ) = y(\phi) = \sigma(w^T \phi) = \frac{1}{1 + \exp(-(w^T\phi))}
\end{align*}

As stated in Bishop, page 208, eq 4.99 and 4.100: The Newton-Raphson update formula for the logistic regression model becomes 

\begin{align*}
\textbf{w}^{new} &= \textbf{w}^{old} - (\boldsymbol{\Phi} ^T \textbf{R} \boldsymbol{\Phi} )^{-1} \boldsymbol{\Phi} ^T (\textbf{y} - \textbf{t})\\
&= (\boldsymbol{\Phi}^T \textbf{R} \boldsymbol{\Phi} )^{-1} \{ \boldsymbol{\Phi}^T \textbf{R}  \boldsymbol{\Phi} \textbf{w}^{(old)} - \boldsymbol{\Phi}^T (\textbf{y} - \textbf{t})  \}\\
&= (\boldsymbol{\Phi}^T \textbf{R} \boldsymbol{\Phi} )^{-1} \boldsymbol{\Phi}^T \textbf{R} \textbf{z} 
\end{align*}

where \textbf{z} is an N-dimensional vector with elements

\begin{align*}
\textbf{z} = \boldsymbol{\Phi} \textbf{w}^{old} - \textbf{R}^{-1} (\textbf{y} -\textbf{t})
\end{align*}



The following Python code converges after 6 iterations towards $\hat{\textbf{w}}^T = [  9.78227684, -21.73839298]$ which is pretty close to the optimal solution.


\begin{lstlisting}[language=Python]
# Exercise 1.1.2

def sigmoid(x):
    """ The standard logistic function. np.exp also accepts arrays"""
    return 1.0 / (1 + np.exp(-x))

def gradient_of_error(phi, y, t):
    """ Gradient (first-order derivatives) of the error function, see Bishop page 207, eq. 4.96 """
    return np.dot(phi.T, y - t)

def hessian_of_error(phi, y):
    """ Hessian (second-order derivatives) of the error function, see Bishop page 207, eq. 4.97 """
    R = np.diag(np.ravel(y * (1 - y)))
    return np.dot(phi.T, np.dot(R, phi))

def cross_entropy_error(y, t):
    # Implementation of cross entropy error = E(w)
    result = 0
    for n in range(0, t.shape[0]):
        result += t[n] * np.log(y[n]) + (1 - t[n]) * np.log(1 - y[n])
    return -result

w = np.array([ [1.0], [1.0] ]) # two-dimensional weight vector
x = np.array([0.3, 0.44, 0.46, 0.6])
t = np.array([ [1], [0], [1], [0] ]) # targets
phi = np.array([ [1, x_element] for x_element in x ]) # feature vector

for i in range(10):
    y = sigmoid(np.dot(phi, w)) # class estimates
    
    current_gradient = gradient_of_error(phi, y, t)
    current_hessian = hessian_of_error(phi, y)
    
    w = w - np.dot(np.linalg.inv(current_hessian), current_gradient)
    
    print("Iteration {}: \nphi={} \ny={}, \ncurrent_gradient={}, \ncurrent_hessian={}, \nw={}\n".format(
        i, phi, y, current_gradient, current_hessian, w))
    
# Converges after 6 iterations  
# w=[[  9.78227684][-21.73839298]]
\end{lstlisting}

\vspace{1em}

In order to show that $\phi = 0.45$ is indeed the decision boundary, we  show that the probability of a data point with $\phi = 0.45$ is the same for both classes. That means that both classes have a probability of  $0.5$ since $p(C_1 \vert \phi) + p(C_2 \vert \phi) = 1$.

		\begin{align*}
			p(C_1 \vert \phi) = y(\phi) = \sigma (\boldsymbol w^T \phi) = \sigma (\begin{pmatrix} 9.78227684 \\ -21.73839298 \end{pmatrix}^T \begin{pmatrix} 1\\0.45 \end{pmatrix}) \approx 0.50
		\end{align*}

We can also prove that in a slightly different way using the optimal solution:

\begin{align*}
p(C_1 | \phi) &= \frac{1}{1 + \exp(-w^T \phi)}\\
0.5 &= \frac{1}{1 + \exp(-[9.8, -21.7]^T [1, \phi])}\\
0.5 &= \frac{1}{1 + \exp(-9.8 + 21.7 \phi)}\\
2 &= 1 + \exp{-9.8 + 21.7 \phi}\\
1 &= \exp(-9.8 + 21.7 \phi)\\
\ln(1) &= -9.8 + 21.7 \phi\\
0 &= -9.8 + 21.7 \phi\\
21.7 \phi &= 9.8\\
\phi &= \frac{9.8}{21.7} \approx 0.45
\end{align*}


Useful internet resources which helped solving this exercise:

\begin{itemize}
	\item Second Order Optimization - The Math of Intelligence 2: \url{https://www.youtube.com/watch?v=UIFMLK2nj_w}
	\item Logistic Regression - The Math of Intelligence (Week 2): \url{https://www.youtube.com/watch?v=D8alok2P468}
	\item \url{https://thelaziestprogrammer.com/sharrington/math-of-machine-learning/solving-logreg-newtons-method}
	\item \url{http://cs229.stanford.edu/notes/cs229-notes1.pdf}
	\item \url{https://www.stat.cmu.edu/~cshalizi/350/lectures/26/lecture-26.pdf}
	\item \url{https://www.stat.cmu.edu/~cshalizi/402/lectures/14-logistic-regression/lecture-14.pdf}
	\item \url{https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/quadratic-approximations/a/the-hessian}
	\item \url{https://web.stanford.edu/group/sisl/k12/optimization/#!index.md}
	\item \url{http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex4/ex4.html}
\end{itemize}




\textbf{Part 2 - Two-class classification using logistic regression}\\

\begin{figure}[H]
\center
\includegraphics[width=0.4\textwidth]{Images/two-class-dataset.png}
\caption{Two class data for logistic regression}
\label{Fig:two-class-dataset}
\end{figure}

Two-class classification using logistic regression in the IRLS algorith,. Data consists of 1000 pairs $\{ x_1, x_2 \}$ with corresponding class labels $\mathcal{C}_1 = 0$ or $\mathcal{C}_2 = 1$. Load it into Matlab using\\

\hspace{1cm} \texttt{data = load('a010\_irlsdata.txt', '-ASCII');}

\hspace{1cm} \texttt{X = data(:,1:2); Y = data(:,3);}\\

\subsection*{1.2.1}

Make a scatter plot of the data, similar to Figure \ref{Fig:two-class-dataset}. (Have a look at Matlab file \texttt{a010plotideas.m} in Brightspace for some ideas to make such a scatter plot and the plots later on.) Do you think logistic regression can be a good approach to classification for this type of data? Explain why.\\

\textbf{Answer:}\\

The following Python code generates figure \ref{Fig:1_2_1}:

\begin{lstlisting}[language=Python]
# Exercise 1.2.1

data = np.loadtxt("data/a010_irlsdata.txt")

print("Shape of loaded data: {}".format(data.shape))
X = data[:,0:2] # feature vector
print("Shape of feature vector: {}".format(X.shape))

Y = data[:,2:3] # label vector
print("Shape of label vector: {}".format(Y.shape))

# Plot
plt.scatter(X[:,0], X[:,1], c=Y.ravel())
plt.xlabel('X')
plt.ylabel('Y')
plt.savefig('figure_1_2_1.png')
plt.show()
\end{lstlisting}

\vspace{1em}

We do not think that Logistic Regression is a good approach to classify this type of data because the data does not seem to be linearly separable.

The decision boundary drawn by Logistic regression is a linear combination of features and weights denoted by $w_0 + \sum_i w_iX_i$ and therefore is only able to draw a hyperplane through the space to predict one class of each side respectively. The presented data cannot be divided by such a plane except when it would be possible to apply kernel methods like in Support Vector Machines which maps the problem into a new space.\\

See also: \url{https://homes.cs.washington.edu/~marcotcr/blog/linear-classifiers/}


\begin{figure}[H]
\center
\includegraphics[width=0.6\textwidth]{Images/figure_1_2_1.png}
\caption{Scatter plot of two class data of exercise 1 - Part 2}
\label{Fig:1_2_1}
\end{figure}





\subsection*{1.2.2}

Modify the Iterative Reweighted Least Squares algorithm from part 1 to calculate the optimal weights for this data. Use again a dummy basis function. Initialize with the weight vector $\textbf{w}^T = [0, 0, 0]$. With these initial weights, what are the class probabilities according to the logistic regression model (i.e., before optimization)?\\

\textbf{Answer:}\\

\begin{align*}
p(C_1 | \phi) &= \frac{1}{1 + \exp(-w^T \phi)}\\
&= \frac{1}{1 + \exp(-[0.0, 0.0, 0.0]^T [1, \phi, \phi])}\\
&= \frac{1}{1 + \exp([0.0, 0.0, 0.0])}\\
&= \frac{1}{1 + \exp(0)}\\
&= \frac{1}{1 + 1}\\
&= \frac{1}{2}
\end{align*}

According to the logistic regression model with the weight vector $\textbf{w}^T = [0, 0, 0]$, both classes have a probability of 0.5.




\subsection*{1.2.3}

Run the algorithm. Make a scatter plot of the data, similar to figure \ref{Fig:two-class-dataset}, but now with color that represent the data point probabilities $P(C = 1, X_n)$ according to the model after optimization. Compare the cross entropy error with the initial value. Did it improve? Much? Explain your findings.\\

\textbf{Answer:}\\


The following Python code generates figure \ref{Fig:1_2_3}:

\begin{lstlisting}[language=Python]
# Exercise 1.2.3

w = np.array([[0.0], [0.0], [0.0]]) # three-dimensional weight vector
x = X # data
t = Y # targets/labels, has to look like np.array([ [1], [0], [1], [0] ])

print("Shape of x: {}".format(x.shape[0]))
print("Shape of t: {}".format(t.shape[0]))

phi = np.array([ [1, x_element[0], x_element[1]] for x_element in x ]) # feature vector

y = sigmoid(np.dot(phi, w))
print("Cross entropy error before optimization: {}".format(cross_entropy_error(y, t)))

for i in range(0, 1000):
    y = sigmoid(np.dot(phi, w)) # class estimates
    current_gradient = gradient_of_error(phi, y, t)
    current_hessian = hessian_of_error(phi, y)
    
    w = w - np.dot(np.linalg.inv(current_hessian), current_gradient)
    
print("Cross entropy error after optimization: {}".format(cross_entropy_error(y, t)))
print("Weights: {}".format(w))

# Normalised [0,1]
y = (y - np.min(y))/np.ptp(y)

cmap = plt.get_cmap('Blues')
plt.scatter(x[:, 0], x[:, 1], c = np.squeeze(cmap(y)))
plt.xlabel('X')
plt.ylabel('Y')
plt.savefig('figure_1_2_3.png')
plt.show()

# Cross entropy error before optimization: [693.14718056]
# Cross entropy error after optimization: [692.96935948]
# Weights: [[ 0.00440664]
# [-0.02139153]
# [-0.04930069]]
\end{lstlisting}

\begin{figure}[H]
\center
\includegraphics[width=0.6\textwidth]{Images/figure_1_2_3.png}
\caption{Scatter plot of two class data with class probabilities}
\label{Fig:1_2_3}
\end{figure}

The cross entropy error only improved slightly from 693.14718056 to 692.96935948.\\

We think the problem lies within the nature of the data that it is not linearly separable which we can also see in the transparency/probabilities in figure \ref{Fig:1_2_3}. The decision boundary seems to be drawn through the middle (linearly) although the data is not seperable that way. Based on the fact that only linear basis functions are used for this non linearly separable data explains that outcome. A non-linear basis function would probably yield better results.



\subsection*{1.2.4}

Introduce two Gaussian basis functions as features $\phi_1, \phi_2$, similar to Bishop, fig.4.12. Use identical isotropic covariance matrices $\Sigma = \sigma^2 I$ with $\sigma^2 = 0.2$, and center the basis functions around $\mu_1 = (0,0)$ and $\mu_2 = (1,1)$. Make a scatter plot of the data in the feature domain. Do you think logistic regression can be a good approach to classification with these features? Explain why.\\

\textbf{Answer:}\\

The following Python code generates figure \ref{Fig:1_2_4}:

\begin{lstlisting}[language=Python]
# Exercise 1.2.4

# Use identical, isotropic covariance matrices Sigma = sigma^2 * I with sigma^2 = 0.2
# and center the basis functions around mu_1 = (0,0) and mu_2 = (1,1)

def gaussian_basis_function(data, origin, variance):
    sigma = variance * np.identity(np.array(data).shape[0])
    a = (data-origin).T
    normalizer = 1.0 / (2*np.pi)**(a.shape[0]/2) * 1.0 / np.linalg.det(sigma)**(1/2)
    return normalizer * np.exp(-(1.0/2)*np.dot(a.T, np.dot(inv(sigma), a)))

phi = np.array([[1, gaussian_basis_function(xx, (0, 0), 0.2), gaussian_basis_function(xx, (1, 1), 0.2)] for xx in x])

plt.scatter(phi[:, 1], phi[:, 2], c = t.squeeze())
plt.xlabel('X')
plt.ylabel('Y')
plt.savefig('figure_1_2_4.png')
plt.show()
\end{lstlisting}


\begin{figure}[H]
\center
\includegraphics[width=0.6\textwidth]{Images/figure_1_2_4.png}
\caption{Scatter plot of two class data in new feature space using gaussian basis functions}
\label{Fig:1_2_4}
\end{figure}

The new feature space is a lot more suitable to a linear classifier because a linear decision boundary can be drawn which separates most of the two classes from each other, although not every point seems to be clearly separable. Nevertheless, this makes logistic regression an appropriate choice.




\subsection*{1.2.5}

Modify the IRLS algorithm to use the features $\{ \phi_1, \phi_2 \}$ and the dummy basis function. Initialize with the weight vector $\textbf{w}^T = [0,0,0]$.\\
Run the algorithm. Make a scatter plot of the data, similar to Figure \ref{Fig:two-class-dataset}, but now with colors that represent the data point probabilities $P(C = 1 | X_n)$ according to this second model (after optimization). Compare the cross entropy error with the initial value. Did it improve? Much? Explain your findings.\\


\textbf{Answer:}\\

The following Python code generates figure \ref{Fig:1_2_5}:

\begin{lstlisting}[language=Python]
# Exercise 1.2.5

y = sigmoid(np.dot(phi, w))
print("Cross entropy error before optimization: {}".format(cross_entropy_error(y, t)))

for i in range(0, 1000):
    y = sigmoid(np.dot(phi, w)) # class estimates
    current_gradient = gradient_of_error(phi, y, t)
    current_hessian = hessian_of_error(phi, y)
    
    w = w - np.dot(np.linalg.inv(current_hessian), current_gradient)
    
print("Cross entropy error after optimization: {}".format(cross_entropy_error(y, t)))
print("Weights: {}".format(w))

# Normalised [0,1]
y = (y - np.min(y))/np.ptp(y)

cmap = plt.get_cmap('Blues')
plt.scatter(x[:, 0], x[:, 1], c = np.squeeze(cmap(y)))
plt.xlabel('X')
plt.ylabel('Y')
plt.savefig('figure_1_2_5.png')
plt.show()

# Cross entropy error before optimization: [690.64901362]
# Cross entropy error after optimization: [346.50408046]
# Weights: [[  7.10834887]
# [-15.42143347]
# [-15.53830921]]
\end{lstlisting}


\begin{figure}[H]
\center
\includegraphics[width=0.6\textwidth]{Images/figure_1_2_5.png}
\caption{Scatter plot of two class data an their probabilities in new feature space using gaussian basis functions}
\label{Fig:1_2_5}
\end{figure}

The cross entropy error improved significantly to 346.50408046. Figure \ref{Fig:1_2_5} shows that the gaussian basis functions are able to separate a big amount of the data at its origin of (0,0) and (1,1) and only data points in the middle have uncertain probabilities around 0.5. The cross entropy error would probably rise again if the origin of the gaussian basis functions shifted towards other origins.


\section*{Exercise 2 - Neural network regression (weight 5)}

We train a neural network using backpropagation, to learn to mimic a 2D multimodal probability density. First, we implement the network and test its regression capabilities on a standard Gaussian; then we train it on the real data set. Visualization of the network output plays an important role in monitoring the progress.

\subsection*{2.1}

Create a plot of an isotropic 2D Gaussian $y = 3 \cdot \mathcal{N}(\textbf{0}|\frac{2}{5}\textbf{I}_2)$ centered at the origin using the \texttt{meshgrid()}, \texttt{mvnpdf()} and \texttt{surf()} functions. Sample the density at 0.1 intervals over the range $[-2,2] \times [-2,2]$ and store the data in colum vector variables $\textbf{X}$ (2D) and $\textbf{Y}$ (1D).\\

\textbf{Answer:}\\


This can be done by using the following code:

\begin{lstlisting}[language=Python]
def Gaus(X):
    FirstPart = 1/np.power(2*np.pi,len(X)/2)*1/(np.power(np.linalg.det(Sigma),0.5))
    Exp = -(1/2)*np.matmul(np.transpose(X-Mu),np.matmul(np.linalg.inv(Sigma),X-Mu))
    return FirstPart*np.exp(Exp)

def Plot(X,Y,Name,save=False,savename=None,title=None):
    x,y = np.hsplit(X,2)
    x = np.reshape(x,(41,41))
    y = np.reshape(y,(41,41))
    z = np.reshape(Y,(41,41))
    fig = plt.figure(Name,figsize=(32,24))
    ax = fig.gca(projection='3d')
    if(title != None):
        plt.title(title,fontsize=40)
    surf = ax.plot_surface(x, y, z, cmap=cm.coolwarm,linewidth=1, antialiased=True)
    fig.colorbar(surf, shrink=0.5, aspect=5,pad=-0.07)
    ax.view_init(azim=-120,elev = 70)
    ax.set_zlabel('\nY  ',fontsize=50)
    ax.set_ylabel('\n$X_1$  ',fontsize=50)
    ax.set_xlabel('\n\n$X_0$',fontsize=50)
    
    if(save):
        pylab.get_current_fig_manager().window.showMaximized()
        ax.set_xlim3d([-2, 2])
        ax.set_ylim3d([-2, 2])
        ax.set_zlim3d([0, 1.5])
        plt.savefig(savename,dpi=100,layout='tight_layout')
        plt.close(Name)
        return
    plt.show()

Sigma = (2/5)*np.identity(2)
Mu = np.zeros(2)
x = y = np.arange(-2,2+0.1,0.1)
x,y = np.meshgrid(x,y)
X = []
for i in range(len(x)):
    for j in range(len(x[0])):
        X.append(np.array([x[i][j],y[i][j]]))
X = np.array(X)
Y = np.array([Gaus(x) for x in X])

Plot(X,Y,"Plot Gaussian")
\end{lstlisting}

The function "Gaus"(line 1) simply follows the definition of the multi-variable Gaussian as given by equation 2.43 at page 78 of Bishop:

\begin{align}
\mathcal N (x|\mu,\sigma^2) = \frac{1}{(2\pi)^{\frac{D}{2}}} \frac{1}{\abs{\Sigma}^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1} (x-\mu)}\tag{2.43}
\end{align}

For plotting, we need to reshape it because this is the way that matplotlib plots, it takes 2 2D arrays for X and Y and 1 2D array for z. The other lines create the colorbar and axis labels.
The construction with the for-loop ensures that X contains all points required uniquely.
The plot of the Gaussian can be seen at figure \ref{GausPlot}:

\begin{figure}[H]
\includegraphics[width=\textwidth]{Images/Plot_Gaussian.png}
\caption{Gaussian function $Y = 3 \cdot \mathcal{N}(\textbf{0}|\frac{2}{5}\textbf{I}_2)$ graphed.}
\label{GausPlot}
\end{figure}


\subsection*{2.2}

Implement a 2-layer neural network with $D = 2$ input nodes, $K = 1$ output nodes and $M$ hidden nodes in the intermediate layer that can be trained using a sequential error backpropagation procedure, as described in Bishop ยง5.3. Use $tanh(\cdot)$ activation functions for the hidden nodes and a linear activation function (regression) for the output node. Introduce appropriate weights and biases, and set the learning rate parameter $\eta = 0.1$. Initialize the weights to random values in the interval $[-0.5,0.5]$. Plot a 2D graph of the initial output of the network over the same $[-2,2] \times [-2,2]$ grid as the Gaussian (again using \texttt{surf()}).\\

\textbf{Answer:}\\

For the network I created a class in Python:

\begin{lstlisting}[language=Python]
class NeuralNW():
    def __init__(self,X,Y,eta=0.1,D=2,K=1,M=8):
        self.w1 = np.random.random_sample((D+1,M))-0.5
        self.w2 = np.random.random_sample(M+1)-0.5
        self.X = X
        self.Y = Y
        self.TrainX = []
        self.eta = eta
        for x in self.X:
            self.TrainX.append(np.array([1,x[0],x[1]]))
        self.TrainX = np.array(self.TrainX)


    def setWeights(self,w1,w2):
        self.w1 = w1
        self.w2 = w2


    def setNetworkOnce(self,Num):
        self.HiddenLayer = np.tanh(np.dot(self.w1[0], self.TrainX[Num][0]) + np.dot(self.w1[1] , self.TrainX[Num][1]) + np.dot(self.w1[2],self.TrainX[Num][2]))
        self.HiddenLayer = np.insert(self.HiddenLayer,0,1)
        self.Output = np.sum(self.HiddenLayer*self.w2)

    def TrainNetwork(self,Num):
        self.setNetworkOnce(Num)
        Delta2 = self.Output - self.Y[Num]
        Delta1 = (1-np.square(self.HiddenLayer))*self.w2*Delta2
        Der2 = Delta2*self.HiddenLayer
        Delta1 = Delta1[1:]
        Der1 = np.array([Delta1*self.TrainX[Num][0],Delta1*self.TrainX[Num][1] , Delta1*self.TrainX[Num][2]])
        self.w2 -= self.eta*Der2
        self.w1 -= self.eta*Der1
        
    def TrainNetworkAll(self):
        for i in range(len(self.X)):
            self.TrainNetwork(i)
            
    def TrainNetworkRand(self):
        for i in np.random.permutation(len(self.X)):
            self.TrainNetwork(i)
    
    def Distance(self):
        Out = []
        for i in range(len(self.X)):
            self.setNetworkOnce(i)
            Out.append(self.Output)
        Out = np.array(Out)
        return np.sum(np.abs(self.Y - Out))
        
    def PlotOutput(self,Name,save=False,savename=None,title=None):
        Out = []
        for i in range(len(self.X)):
            self.setNetworkOnce(i)
            Out.append(self.Output)
        Out = np.array(Out)
        Plot(self.X,Out,Name,save=save,savename=savename,title=title)
\end{lstlisting}

\subsubsection*{Initialization}

When it is initialized it creates two sets of weights. The first (w1) are the weights form the inputs to the hidden layer. There are D inputs + 1 bias, so there are (D+1) sets of weights leading to the M hidden layers. This I represent in a two dimensional (D+1,M) matrix, where w1[0][5] stands for the weight between $x_0$ (bias) and $z_5$, of the hidden layer.
The hidden layer is connected to the one output by M+1 weights, since we have another bias $z_0$. The fandom\_sample function gives a random number between [0,1] so I lower it by 0.5 to make it between [-0.5,0.5].
Then it stores the X and Y values and for convenience creates an array "self.X" which already has $x_0$ as bias, which is always 1.

\subsubsection*{Set network}
To run something through the model you can set all the layers of the network, including the output, by calling the "setNetworkOnce" function. This runs the nth (given by "Num") entry of the X array. The bias is set at 1 using "np.insert(self.HiddenLayer,0,1)".

\subsubsection*{TrainNetwork \& TrainNetworkAll}

This will run the nth point (given by Num) of the vector X and set the weights according to the 'simple example' given at page 245 of Bishop. TrainNetworkAll will call the function TrainNetwork for all points in X.

\subsection*{Distance}

This will calculate the distance between the output and the target. This is used to numerically evaluate the network's results.

\subsection*{TrainNetworkRand}

This trains the network but selects the data at random, which will be useful soon.

\subsubsection*{PlotOutput}

This will put every point in X through the network and plot it's output. 


The initial output is for M = 8 can be seen in the following figure \ref{First}:

\begin{figure}[H]
\includegraphics[width=\textwidth]{Images/First.png}
\caption{Inital output of the network for m = 8.}
\label{First}
\end{figure}






\subsection*{2.3}

Train the network for $M = 8$ hidden nodes on the previously stored $\textbf{X}$ and $\textbf{Y}$ values (the $\{ x_1, x_2 \}$ input coordinates and corresponding output probability density $y$), by repeatedly looping over all datapoints and updating the weights in the network after each point. Repeat for at least 500 complete training cycles and monitor the progress of the training by plotting the output of the network over the $\textbf{X}$ grid after each full cycle. Verify the output starts to resemble the Gaussian density after some 200 cycles (all be it with lots of 'wobbles').\\


\textbf{Answer:}\\


The code I used for this is:


\begin{lstlisting}[language=Python]
Network = NeuralNW(X,Y,eta)
Now = time()

Network.PlotOutput("First")

for i in range(2000):
    if (i == 199):
        Network.PlotOutput("Second")
        print(time()-Now)
    Network.TrainNetworkAll()

Network.PlotOutput("Third")
print(time()-Now)
\end{lstlisting}

First we create an instance of the Network class. Then we plot the initial output and run it for some iterations, this case, 2000. We plot the intermediate result at 200 times.Then we plot the final output. In 200 iterations we got the following output, figure \ref{Second}

\begin{figure}[H]
\includegraphics[width=\textwidth]{Images/Second.png}
\caption{Output after 200 iterations of training.}
\label{Second}
\end{figure}


Then we ran it for 2000 iterations and the results you can see at figure \ref{Third}

\begin{figure}[H]
\includegraphics[width=\textwidth]{Images/Third.png}
\caption{Output after 2000 iterations of training.}
\label{Third}
\end{figure}

So it reassembles the function after 200 iterations and a lot more after 2000. 

\subsection*{2.4}

Permute the $\textbf{X}$ and $\textbf{Y}$ arrays to a random order using the \texttt{randperm()} function, keeping corresponding $x$ and $y$ together. Repeat the network training session using this randomized data set. Verify that convergence is now much quicker. Can you understand why? Try out the effect of different numbers of hidden nodes, different initial weights and different learning rates on speed and quality of the network training. Explain your results.
\textbf{Answer:}\\


We permute by doing the following:


\begin{lstlisting}[language=Python]
    def TrainNetworkRand(self):
        for i in np.random.permutation(len(self.X)):
            self.TrainNetwork(i)
\end{lstlisting}

This is a function of the NeuralNetwork class. Instead of training every data point from start to finish this randomizes the order, which is equal to randomizing X and Y. (It's a little bit better since it randomizes every time it's training)

We get an indication of the convergence by using the following error function:

\begin{lstlisting}[language=Python]
    def Distance(self):
        Out = []
        for i in range(len(X)):
            self.setNetworkOnce(i)
            Out.append(self.Output)
        Out = np.array(Out)
        return np.sum(np.abs(self.Y - Out))
\end{lstlisting}

This takes the output for every point and calculates how far it is from the target, and sums it. The following code we used to show the convergence for a random permuation is indeed faster:

\begin{lstlisting}[language=Python]
NetworkRand = NeuralNW(X,Y)
for i in range(20):
    NetworkRand.TrainNetworkRand()
print("Error for the random training = ",NetworkRand.Distance())


NetworkNorm = NeuralNW(X,Y)
for i in range(20):
    NetworkNorm.TrainNetworkAll()
print("Error for the sequential network = ",NetworkNorm.Distance())
\end{lstlisting}

A typical result would be something like ~50 for the randomized training and ~100-250 for the sequential training. By choosing random points the difference between the output and the desired output is higher, therefore more training can be done. With the desired output being close to the target not much training will be done since the gradient is not as high. An example output is:

\begin{quote}
Error for the random training =  55.80648504579132\\
Error for the sequential network =  128.7314508706291
\end{quote}

\subsubsection*{Different parameters}

We've tested the convergence by choosing different parameters and looking at the Error after 100 iterations of random training.
First we've tested for the amount of nodes M using the following code:


\begin{lstlisting}[language=Python]
ErrorsM = []
Mlist = np.linspace(1,50,50,dtype=np.int)

for M in Mlist:
    print(M)
    NetworkTestM = NeuralNW(X,Y,eta)
    for i in range(100):
        NetworkTestM.TrainNetworkRand()
    ErrorsM.append(NetworkTestM.Distance())

plt.figure("Erors M")
plt.plot(Mlist,ErrorsM)
plt.xlabel('M')
plt.ylabel('Error')

plt.figure("Time M")
plt.plot(Mlist,TimeList)
plt.xlabel('M')
plt.ylabel('t(s)')
\end{lstlisting}

This creates an list of 1,2,...,50, to test Neural networks with these amount of hidden layers. Then it trains the network and append the error. It also keeps track of how long the training took. Finally it plots the error and the execution times.
The result can be viewed in figure \ref{Merror}.



\begin{figure}[H]	
	\begin{subfigure}{0.45\textwidth} % width of left subfigure
		\includegraphics[width=\textwidth]{Images/Errors_M.png}
	\end{subfigure}
	\begin{subfigure}{0.45\textwidth} % width of left subfigure
		\includegraphics[width=\textwidth]{Images/Time_M.png}
	\end{subfigure}
	\caption{The error and time for various amount of hidden layers M. The time taken is fairly constant and the error seems to be constant after ~10-20 hidden layers. So it's best to take around 20 hidden layers.}
	\label{Merror}
\end{figure}


We also tested the effect of different $\eta$, for which we took a logarithmically, meaning that the log of the different $\eta's$ reveals a linear scale.

We used the following code:

\begin{lstlisting}[language=Python]
Errorseta = []
EtaLog = np.linspace(-4,2,50)

Etas = np.power(10,EtaLog)

for eta in Etas:
    print(eta)
    NetworkTesteta = NeuralNW(X,Y,eta=eta)
    for i in range(100):
        NetworkTesteta.TrainNetworkRand()
    Errorseta.append(NetworkTesteta.Distance())

plt.figure("Errors eta")
plt.plot(Etas,Errorseta)
plt.xlabel('$\eta$')
plt.ylabel('Error')
\end{lstlisting}

So we took 50 different values for $\eta$ between $10^-4$ and $10^2$. The plot we got from this calculation is figure \ref{EtaErr}:

\begin{figure}[H]
\includegraphics[width=\textwidth]{Images/ErrorVarEta.png}
\caption{Errors for different values of $\eta$. A value around 0.1-1.5 seems to be the best.}
\label{EtaErr}
\end{figure}

Finally to test the influence of the starting values of the weights we we took a random value between $[-0.5+i,0.5+i]$ with i varying from -5 to 5. So we can test if a lower random weight or a higher random weight would be beneficial. We did it using the following code:

\begin{lstlisting}[language=Python]
ErrorW = []
Weights = []
D = 2
M = 8
for i in range(-5,5):
    Weights.append([])
    for j in range(10):
        w1 = np.random.random_sample((D+1,M))-0.5+i
        w2 = np.random.random_sample(M+1)-0.5+i
        Weights[-1].append((w1,w2))


for i, partWeight in enumerate(Weights):
    print(i)
    ErrorW.append(0)
    for w1,w2 in partWeight:
        NetworkTestW = NeuralNW(X,Y)
        NetworkTestW.setWeights(w1,w2)
        for i in range(100):
            NetworkTestW.TrainNetworkRand()
        ErrorW[-1] += NetworkTestW.Distance()

ErrorW = np.array(ErrorW)/10

WPlot = range(-5,5)
plt.figure("Errors in W")
plt.plot(WPlot,ErrorW)
plt.xlabel('Inital weights bias')
plt.ylabel('Error')
\end{lstlisting}

We tested very different w 10 times to reduce the bias for taking random values for w. The number i we called the 'bias'. The code from 30-32 is part of the Neuralnetwork class and is used to set the weights.
The plot we got from this can be seen at figure \ref{WDif}.

\begin{figure}[H]
\includegraphics[width=\textwidth]{Images/Errors_in_W.png}
\caption{The bias on the random value of the inital weights. A bias of 0 seems to be the best.}
\label{WDif}
\end{figure}

To summarize: Only the amount of hidden networks could be a little bit higher in order to get a better convergence, but the values for w and $\eta$ given in the assignment seemed to be quite close to the optimal, as we can conclude from our tests.


\begin{figure}[H]
\center
\includegraphics[width=0.4\textwidth]{Images/multi-modal.png}
\caption{Multi-modal probability density}
\label{Fig:multi-modal}
\end{figure}



After these preliminaries we are now going to train the network on the real data set.\\
Load the data using\\


\hspace{1cm} \texttt{data = load('a017\_NNpdfGaussMix.txt', '-ASCII');}

\hspace{1cm} \texttt{X = data(:,1:2); Y = data(:,3);}\\







\subsection*{2.5}

Create a $2D$-plot of the target probability density function. Notice that the data is in the correct sequence to use in \texttt{surf()}.\\

\textbf{Answer:}\\

This has been done with the following code:

\begin{lstlisting}[language=Python]
Data = np.genfromtxt('data/a017_NNpdfGaussMix.txt')
X2 = Data[:,0:2]
Y2 = Data[:,2]
Plot(X2,Y2,"2.5")
\end{lstlisting}


Figure \ref{2.5} shows the plot:

\begin{figure}[H]
\includegraphics[width=\textwidth]{Images/2,5.png}
\caption{Plot of the data.}
\label{2.5}
\end{figure}






\subsection*{2.6}

Train the network on this data set. Use at least 40 hidden nodes and a learning rate parameter no higher than $\eta = 0.01$. Make sure the input data is properly randomized. Run the training phase for at least 2000 complete cycles and follow the progress by plotting the updated network output after every 20 full cycles. How does the final output of the network compare to the target distribution in the data? Explain. How could you improve the neural network in terms of speed of convergence and/or quality of the approximation?


\textbf{Answer:}\\


The following code has been used to produce the result:

\begin{lstlisting}[language=Python]
RealDataNNW = NeuralNW(X2,Y2,eta=0.01,M=80)
for i in range(2000):
    
    if (i % 20 == 0):
        RealDataNNW.PlotOutput("Real" + str(i),save=True,savename="../latex/Images/Final2/" + str(i) + ".png",title="Iteration " + str(i))
    RealDataNNW.TrainNetworkRand()


RealDataNNW.PlotOutput("Real" + str(i),save=True,savename="../latex/Images/Final2/" + str(i+1) + ".png",title="Iteration " + str(i+1))
\end{lstlisting}


The code trains the network 2000 times and saves the output of the network every 20th time. 
The images can be found in the folder "2Final". We have also made an video which shows all the 101 pictures.
The end result looks similar as the target, however, the small higher part of the target is not reproduced so well.
The neural network is too simple to capture the picture perfectly. It works well when there is a smooth change but abrupt changes are very hard to model.
The input is just the X values, so the output change in a similar as it's input, therefore, less abrupt changes in the target are easier to model.
We could improve the accuracy and convergence by, for example, fine-tuning the learning rate $\eta$. We could also chose a more complex neural network, one that, for instance, takes all the X values as input (as if it were "pixels"). The following picture \ref{Final2} shows the final output:

\begin{figure}[H]
\includegraphics[width=\textwidth]{Images/2Final/2000.png}
\caption{Final output of the neural network fitting the Multi-modal probability density.}
\label{Final2}
\end{figure}



\section*{Exercise 3 - Gaussian processes (weight 5)}


\textbf{Part 1 - Sampling from Gaussian stochastic processes}\\

One widely used kernel function for Gaussian process regression is given by the exponential of quadratic form, with the addition of constant and linear terms (eq. 6.63 Bishop):

\begin{eqnarray} \label{eq:5}
k(\textbf{x}, \textbf{x}') = \theta_0 \exp(- \frac{\theta_1}{2} || \textbf{x} - \textbf{x}' ||^2) + \theta_2 + \theta_3 \textbf{x}^T \textbf{x}'
\end{eqnarray}

We denote by $\boldsymbol{\theta}  = (\theta_0, \theta_1, \theta_2, \theta_3)$ the hyperparameter vector governing the kernel function $k$.

\subsection*{3.1.1}

Implement the kernel given by Equation (\ref{eq:5}) in Matlab as a function of $\textbf{x}, \textbf{x}'$ and $\boldsymbol{\theta}$. Note that $\textbf{x}$ can have any dimension.\\

\textbf{Answer:}\\


\begin{lstlisting}[language=Python]
def k(x,x2,T):
    return T[0]*np.exp(-(T[1]/2)*np.power(np.abs(x-x2),2))+T[2]+T[3]*x*x2
\end{lstlisting}
=======



\subsection*{3.1.2}

We first consider the univariate case. For the parameter values $\boldsymbol{\theta} = (1,1,1,1)$ and $N = 101$ equally spaced points $\textbf{X}$ in the interval $[-1,1]$, compute the Gram matrix $\textbf{K(X,X)}$ (eq. 6.54 Bishop). What is the dimension of \textbf{K}? How can we show that \textbf{K} is positive semidefinite?\\



Note: Even when \textbf{K} is positive definitive, some of its eigenvalues may be too small to accurately compute (same for the determinant). This may pose a problem when generating a multivariate Gaussian distribution using \textbf{K} as its covariance matrix. You can alleviate this issue by adding a small diagonal term to \textbf{K}.\\


\textbf{Answer:}\\



A Gram matrix is a $N \times N$ symmetric matrix (Bischop page 293, just before (6.6)). So in this case, the matrix is $101 x 101$.

A Gram matrix can be computed usinng the following code:

\begin{lstlisting}[language=Python]
def GramMatrix(X,T):
    N = len(X)
    Gram = np.zeros((N,N))
    for i in range(N):
        for j in range(N):
            Gram[i][j] = k(X[i],X[j],T)
    return Gram
\end{lstlisting}

It first creates an empty $N \times N$ matrix, then fills it using the kernel function as required by the definition of the matrix. To show that K is positive semidefinite we could calculate the eigenvalues of the matrix and see if they are all $\geq 0$. This can be done with the following code:

\begin{lstlisting}[language=Python]
def SemiDef(Matrix):
    FixedMatrix = Matrix + 1e-10*np.identity(len(Matrix))
    EigVals = np.linalg.eigvals(FixedMatrix)
    Lowest = np.min(EigVals)
    IsSemi = np.real(Lowest)>=0
    return IsSemi
print(SemiDef(Gram))
\end{lstlisting}

This first adds a small value of $10^{-10}$ to the diagonal, as suggested by the note. Then it calculates it's eigenvalues, takes the minimum and looks if the real part is greater then 0. (by default the eigenvalue is complex since any matrix has eigenvalues as a complex number, since some polynomials of the characteristic polynomial of a matrix (like $x^2+1$) cannot be factorized using only real numbers.) Though since K is a symmetric real matrix, the matrix is Hermitian and the eigenvalues are all real, so the imaginary part is due to numerical inaccuracies and it needs to be neglected in order to see if the lowest value is greater or equal then 0.\footnote{That a symmetric real matrix is Hermitian and that Hermitian matrices have real eigenvalues is shown at wikipedia at \url{https://en.wikipedia.org/wiki/Hermitian_matrix}}The printstatement was "true", so it is a positive semidefinite matrix.




\subsection*{3.1.3}

We  will now use the previously computed matrix \textbf{K(X,X)} to produce samples from the Gaussian process prior $\textbf{y(X)} \sim  \mathcal{N}(\textbf{0, K(X,X)})$, with \textbf{X} being the previously determined N equally spaced points. Generate five functions \textbf{y(X)} with Matlab and plot them against the N input values \textbf{X}. Repeat the process (remember to compute a new \textbf{K} each time) for the hyperparameter configurations from Bishop, Figure 6.5:

\begin{align*}
\boldsymbol{\theta} \in \{(1,4,0,0),(9,4,0,0),(1,64,0,0),(1,0.25,0,0),(1,4,10,0),(1,4,0,5)\}.
\end{align*}

Describe the differences between the plots. Explain in which way each of the kernel parameters affects the generated samples.\\


\textbf{Answer:}\\


We've done this using the following code:


\begin{lstlisting}[language=Python]
Theta = np.array([[1, 4, 0, 0], [9, 4, 0, 0], [1, 64, 0, 0], [1, 0.25, 0, 0],
                  [1, 4, 10, 0], [1, 4, 0, 5]])
for j in range(len(Theta)):
    T = Theta[j]
    K = GramMatrix(X,T) 
    for i in range(5):
        TestGaus = np.random.multivariate_normal([0]*101,K)
        Ax = plt.subplot(2, 3, j + 1)
        Ax.set_xticks(np.round(np.linspace(-1, 1, 5), 2))
        for tick in Ax.xaxis.get_major_ticks():
            tick.label.set_fontsize(30)
        for tick in Ax.yaxis.get_major_ticks():
            tick.label.set_fontsize(30)
        Ax.set_xlim([-1,1])
        Ax.plot(X,TestGaus)
        Ax.set_title(str(T),size=30)
\end{lstlisting}

First we save the different Theta's. Then we use the multivariate.normal function to randomly take a value from the multivariate Gaussian $p(\textbf y) = \mathcal N (\textbf y | 0,\textbf K)$. We plot these in a subplot and then make sure the plot looks better by adjusting the size of the labels and set the xlimit.

What we got can be seen in figure \ref{3.1.3}:

\begin{figure}[H]
\includegraphics[width=\textwidth]{Images/3,1,3.png}
\caption{Samples from Gaussian process prior using the Gram matrix with the $\theta's$ given.}
\label{3.1.3}
\end{figure}

You can see a few differences: some plots have a higher maximum value. Others have lines that are more straight while others have lines that are more squiggly. For example, compare [1. 64. 0. 0.] with [1.0.25 0. 0.]. What differs is the second $\theta_1$. The exponent function is then more sensitive for changes in the input and thus you get more squiggles. 

%Say something about the parameters!
\subsection*{3.1.4}

We now move to the bivariate case. Instead of an interval, we now consider a 2-D grid of equally spaced points of size $N = 21 \times 21$ in $[-1,1] \times [-1,1]$. We collect all these grid points in a data matrix \textbf{X}, where each one of the 441 observations has two dimensions. What is the dimension of \textbf{K} now? What does this tell you about the scalability of sampling multivariate functions from Gaussian processes in higher dimensions?\\

\textbf{Answer:}\\

The Gram matrix K is now a 441 x 441 matrix, with 194,481 entries. This tells that this doesn't scale so well in higher dimensions, for the size of the matrix grows very fast. (The amount of points grows to the power of the amount of dimensions, or: $\#points = n^{2D}$, with n the amount of points in one dimension and D the amount of dimensions. The factor two appears for the Gram Matrix is a square matrix. 






\subsection*{3.1.5}

Using the same kernel from (\ref{eq:5}), compute the Gram matrix \textbf{K(X,X)} on the grid for each hyperparameter configuration $\boldsymbol{\theta} \in \{(1, 1, 1, 1), (1, 10, 1, 1), (1, 1, 1, 10)\}$. For each \textbf{K}, generate and plot four random surfaces from the Gaussian process prior $\textbf{y(X)} $~$ \mathcal{N}\textbf{(0, K(X,X)})$. Compare the observed differences to the univariate case.\\


\textbf{Answer:}\\


We used the following code:


\begin{lstlisting}[language=Python]
Theta = np.array([[1, 1, 1, 1], [1, 10, 1, 1], [1, 1, 1, 10]])
for T in Theta:
    fig = plt.figure(str(T))
    fig.suptitle(str(T),fontsize=50)
    Gram2 = GramMatrix(X,T)
    for i in range(4):     
        Z = np.random.multivariate_normal([0]*441,Gram2)
        Z = np.reshape(Z,(21,21))
        ax = fig.add_subplot(221+i,projection='3d')
        ax.set_xticks(np.round(np.linspace(-1, 1, 5), 2))
        ax.set_yticks(np.round(np.linspace(-1, 1, 5), 2))
        surf = ax.plot_surface(x, y, Z, cmap=cm.coolwarm,linewidth=1, antialiased=True)
        fig.colorbar(surf, shrink=0.5, aspect=5,pad=-0.07)
        ax.view_init(azim=-120,elev = 70)
    pylab.get_current_fig_manager().window.showMaximized()
    plt.subplots_adjust(wspace=0, hspace=0)
\end{lstlisting}






This resulted in the following images:
\begin{figure}[H]
\includegraphics[width=\textwidth]{Images/[1_1_1_1].png}
\end{figure}

\begin{figure}[H]
\includegraphics[width=\textwidth]{Images/[_1_10__1__1].png}
\end{figure}

\begin{figure}[H]
\includegraphics[width=\textwidth]{Images/[_1__1__1_10].png}
\end{figure}


We see again that a high value for $\theta_2$ results in more squiggles, though now they are 2-dimensional.


\textbf{Part 2 - Gaussian processes for regression}\\

We would like to apply Gaussian process models to the problem of regression (Bishop 6.4.2). We consider a noisy model of the form 

\begin{align*}
t_n = y_n + \epsilon_n,
\end{align*}

where $y_n = y(x_n)$ and $\epsilon_n$ are i.i.d. samples from a random noise variable on the observed target values. Furthermore, we assume that the noise process has a Gaussian distribution given by:

\begin{eqnarray}
p(t_n | y_n) = \mathcal{N}(t_n | y_n, \beta^{-1})
 \label{eq:6}
\end{eqnarray}

Going back to a one-dimensional input space, we consider the following training data consisting of four data points 

\begin{align*}
\mathcal{D} \{ (x_1 = -0.5, t_1 = 0.5), (x_2 = 0.2, t_2 = -1), (x_3 = 0.3, t_3 = 3),(x_4 = -0.1, t_4 = -2.5) \}
\end{align*}


\subsection*{3.2.1}

Just as before, compute the Gram matrix of the training data for $\boldsymbol{\theta} = (1,1,1,1)$. Then, taking $\beta = 1$ in Equation \ref{eq:6}, compute the covariance matrix \textbf{C} corresponding to the marginal distribution of the training target values: $p(\textbf{t}) = \mathcal{N}(\textbf{t} | \textbf{0, C})$.\\

\textbf{Answer:}\\


Given equation (6.61) and (6.62) from Bishop:

\begin{align}
p(t) &= \int p(\textbf t | \textbf y)p(\textbf y) d \textbf y = \mathcal N (\textbf t| \textbf 0 , \textbf C) \tag{6.61}\\
C(x_n,x_m) &= k(x_n,x_m) + \beta^{-1}\delta_{nm} \tag{6.62}
\end{align}

Therefore we simply get:

\begin{equation}
C = K + \beta^{-1}I_N\label{C}
\end{equation}

The code to get C is shown in 3.2.2 . 



\subsection*{3.2.2}

Using the previous results, compute the mean and the covariance of the conditional distribution $p(t | \textbf{t})$ of a new target value $t$ corresponding to the input $x = 0$. Which equations from Bishop do you need?\\
\textbf{Answer:}\\


We can use equation (6.66) and (6.67):

\begin{align}
m(\textbf x_{N+1}) &=\textbf k^T \textbf C_N^{-1} \textbf t \tag{6.66}\label{666}\\
\sigma^2(\textbf x_{n+1}) &= c-\textbf k^T \textbf C_N^{-1}\textbf k \tag{6.67}
\end{align}

Here c is defined as:

\begin{equation}
c = k(\textbf x_{N+1},\textbf x_{N+1})+\beta^{-1}
\end{equation}

The mean for the new input $x=0$ we got using the following code:

\begin{lstlisting}[language=Python]
def MeanSigma(X,xnew,C,t,Theta):
    N = len(x)
    K = np.zeros(N)
    for i in range(N):
        K[i] = k(x[i],xnew,Theta)
    Cinv = np.linalg.inv(C)
    Mean = np.dot(K.transpose(),np.dot(Cinv,t))[0]
    c = k(xnew,xnew,Theta)+1
    Sigma = c - np.dot(K.transpose(),np.dot(Cinv,K))
    return Mean,Sigma
Theta = np.array([1,1,1,1])
D = np.array([[-0.5,0.5],[0.2,-1],[0.3,3],[-0.1,-2.5]])
x,t = np.hsplit(D,2)
Gram = GramMatrix(x,Theta)
C = Gram + np.identity(len(Gram))
print(MeanSigma(x,0,C,t,Theta))
\end{lstlisting}

In this function MeanSigma we calculate the mean and sigma using the formula \ref{666} and \ref{C}. As a result we got a mean of -0.0251573646438068 and $\sigma^2$ of 1.2356947349141525.


\subsection*{3.2.3}

Does the mean of the conditional distribution $p(t | \textbf{t})$ go to zero in the limit $x \to \pm \infty$? If so, explain why this happens. If not, how would you set the parameters $\boldsymbol{\theta}$ of the kernel function to make it happen?\\

\textbf{Answer:}\\

The mean doesn't go to 0, instead, it grows to infinity. This makes sense, for the last part of k is $T_{last}=\theta_3 x^Tx'$. When x gets very large then this term also gets very large. And in the mean this comes back because you need to calculate k of every current point with the new point. If somehow all the values of k are 0, then $\textbf k^T \textbf C_N^{-1} \textbf t = \textbf 0^T \textbf C_N^{_1} \textbf t = 0$.
Here $\textbf 0$ is the 0 vector of length N. The exponential part of K ($\theta_0 \exp(- \frac{\theta_1}{2} || \textbf{x} - \textbf{x}' ||^2)$) will go to zero in the limit of $x\rightarrow \pm \infty$.
 And the constant part $\theta_3$ will off course not change for any x. So we need to set $\theta_3$ and $\theta_4$ to 0,
  while the other parameters can have any finite value. For example, a $\boldsymbol\theta = [1,1,0,0]$ will do.


\section*{Exercise 4 - EM and doping (weight 5)}

In a certain hypothetical sport, banned substance '\textbf{X}' has become popular as a performance enhancing drug, as its presence is hard to establish in blood samples directly. Recently, it has been discovered that users of the drug tend to show a strong positive correlation between concentrations of two other quantities, $x_1$ and $x_2$, present in the blood. In contrast, 'clean' athletes tend to fall in one of two or three groups, that either show no or a negative correlation between $x_1$ and $x_2$. Unfortunately, as each sample contains only a single, instantaneous, measurement for each variable, it is not possible to establish this correlation from the sample. However, in many cases it is possible to distinguish to which \textit{class} a certain sample belongs by also looking at the values of two other measured variables, $x_3$ and $x_4$: certain combinations of measured values are often typical for one class but highly unusual for others.\\
After a high profile event, a large scale test has resulted in 2000 samples. Rumours suggest the number of positives could be as high as 20\%. However, the exact relationship between different classes and typical $x$ values is still not clear. This is where the EM-algorithm comes in...\\

The blood sample measurements are modelled as a mixture of $K$ Gaussians, one for each class

\begin{eqnarray} \label{eq:7}
p(x | \mu, \Sigma, \pi) = \sum^K_{k = 1} \pi_k \mathcal{N}(x | \mu_k, \sigma_k)
\end{eqnarray}

where $\textbf{x} = [x_1, x_2, x_3, x_4]$ represents the values for the measured quantities in the blood sample, $\mu = \{ \mu_1, ..., \mu_K\}$ and $\Sigma = \{ \Sigma_1, ..., \Sigma_K\}$ are the means and covariance matrices of the Gaussians for each class, and $\pi = \{ \pi_1, ..., \pi_K\}$ are the mixing coefficients in the overall data set.\\

Load the data using\\

\hspace{1cm} \texttt{data = load('a011\_mixdata.txt', '-ASCII');}\\

and set N to the number of datapoints and D to the number of variables in the dataset X.

\subsection*{4.1}

Try to give an estimate of the number, size and shape of the classes in the data, by plotting the distribution of the variables, e.g. using \texttt{hist()}, \texttt{scatter()} or \texttt{scatter3()}.\\

\textbf{Answer:}\\

The following Python code generates figure \ref{Fig:4_1_a} and \ref{Fig:4_1_b}:

\begin{lstlisting}[language=Python]
# Exercise 4.1

data = np.loadtxt("data/a011_mixdata.txt")

N = data.shape[0]
print("Number of data points 'N': {}".format(N))

D = data.shape[1]
print("Number of variables 'D': {}".format(D))

mean_1 = np.mean(data[:,0], axis=0)
mean_2 = np.mean(data[:,1], axis=0)
mean_3 = np.mean(data[:,2], axis=0)
mean_4 = np.mean(data[:,3], axis=0)

print("Mean of x_1: {}".format(mean_1))
print("Mean of x_1: {}".format(mean_2))
print("Mean of x_1: {}".format(mean_3))
print("Mean of x_1: {}".format(mean_4))

min_1 = np.min(data[:,0])
max_1 = np.max(data[:,0])
min_2 = np.min(data[:,1])
max_2 = np.max(data[:,1])
min_3 = np.min(data[:,2])
max_3 = np.max(data[:,2])
min_4 = np.min(data[:,3])
max_4 = np.max(data[:,3])
print("Min and max of x_1: {}, {}".format(min_1, max_1))
print("Min and max of x_2: {}, {}".format(min_2, max_2))
print("Min and max of x_3: {}, {}".format(min_3, max_3))
print("Min and max of x_4: {}, {}".format(min_4, max_4))

# See: https://seaborn.pydata.org/generated/seaborn.pairplot.html
# Pairplot shows you the correlation between two variables
# The diagonal just shows the histogram of one variable
df = pd.DataFrame(data, columns=["$x_1$", "$x_2$", "$x_3$", "$x_4$"])
sns.set(style="ticks", color_codes=True)
sns.pairplot(df)
plt.savefig('figure_4_1_a.png')
plt.show()


sns.scatterplot(data=data)
plt.xlabel('Datapoint index')
plt.ylabel('value')
plt.savefig('figure_4_1_b.png')
plt.show()

# Number of data points 'N': 2000
# Number of variables 'D': 4
# Mean of x_1: 11.8984181195
# Mean of x_1: 1.9129965287477
# Mean of x_1: 0.37928800998915
# Mean of x_1: 3.7035525292735003
# Min and max of x_1: 11.315281, 12.37204
# Min and max of x_2: -4.8354242, 4.8635103
# Min and max of x_3: -2.9653298, 3.1605988
# Min and max of x_4: -3.2219019, 10.582742
\end{lstlisting}



\begin{figure}[H]
\center
\includegraphics[width=0.7
\textwidth]{Images/figure_4_1_a.png}
\caption{Histograms and correlations between two variables}
\label{Fig:4_1_a}
\end{figure}

\begin{figure}[H]
\center
\includegraphics[width=0.8
\textwidth]{Images/figure_4_1_b.png}
\caption{Scatter plot of all four variable values along their their datapoint index}
\label{Fig:4_1_b}
\end{figure}


\subsection*{4.2}

Implement an EM-algorithm using the description and formulas given in Bishop, ยง9.2.2. Use variable K for the number of classes and choose a priori equal mixing coefficients $\pi_k$. Initialize the means $\boldsymbol{\\mu}_k$, to random values around the sample mean of each variable, e.g. set $\mu_{k,1}$ to  $\bar{x}_1 + [-1 \leq \epsilon \leq +1]$. Initialize the $\Sigma_k$ to diagonal matrices with reasonably high variances, e.g. $4*\texttt{rand()}+2$, to avoid very small responsibilities in the first step. Make sure the EM-loop runs over at least 100 iterations. Display relevant quantities, at least the log likelihood (9.28), after each step so you can monitor progress and convergence. Write a plot routine that plots the $x_1, x_2$ coordinates of the data points, and color each data point according to the most probable component according to the mixture model.\\

\textbf{Answer:}\\

The following Python code generated figure \ref{Fig:log_likelihoods} and \ref{Fig:responsibilities}:

\begin{lstlisting}[language=Python]
# Exercise 4.2

def gaussian(x, mu, sigma):
    '''
    Implementation of the gaussian pdf from scratch because scipy throws LinAlgErr when
    covariance matrix is near singular when using multivariate_normal
    See: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html
    '''
    normalizer = 1. / (2*np.pi)**(x.shape[0]/2) * 1. / np.sqrt(np.linalg.det(sigma))
    return normalizer * np.exp(-1./2 * np.dot(x - mu, np.dot(np.linalg.inv(sigma), x - mu)))

def calculate_log_likelihood(data, pi, mu, sigma, K):
    return np.sum([np.log(np.sum([pi[k] * gaussian(x, mu[k], sigma[k]) for k in range(0, K)])) for x in data])


def expectation_maximization(data, K, iterations=100, early_stopping=False, epsilon=0.01):
    # ******1. Initialize values and evaluate log likelihood ****** 
    
    N = data.shape[0]
    D = data.shape[1]
    
    # equally likely mixing coefficients
    pi = np.ones(K)/K 
    # means, random values around the sample means from -1 to +1
    mu = np.random.uniform(-1, 1, size=(K, D)) + data.mean(axis=0) 
    # covariance matrices, diagonal matrices with variance = 4 * random + 2
    sigma = [np.diagflat(np.random.rand(1, data.shape[1]) * 4 + 2) for _ in range(0, K)]
    # responsibilities initiated to 0
    gamma = np.zeros((data.shape[0], K))
    
    log_likelihoods = []
    
    log_likelihoods.append(calculate_log_likelihood(data, pi, mu, sigma, K))
    print("Initial log likelihood: {:.4f}".format(log_likelihoods[0]))
    
    for i in range(1, iterations+1):
    
        
        # ****** 2. E-step: evaluate responsibilities ****** 
        for k in range(0, K):
            gamma[:, k] = [pi[k] * gaussian(sample, mu[k], sigma[k]) for sample in data]

        gamma = gamma / np.repeat(np.sum(gamma, 1)[np.newaxis], K, axis=0).T
    

        # ****** 3. M-step: Re-estimate parameters ****** 
        N_k = np.sum(gamma, 0)
        
        for k in range(0, K):
            mu[k] = 1./N_k[k] * np.dot(gamma[:, k], data)
            sigma[k] = 1./N_k[k] * np.dot(gamma[:, k], (data - mu[k])[np.newaxis].T * (data - mu[k])[np.newaxis])

        pi = N_k / N
        
        
        # ****** 4. Evaluate log likelihood ****** 

        log_likelihoods.append(calculate_log_likelihood(data, pi, mu, sigma, K))
        print("Iteration {:d}: Log likelihood = {:.2f}".format(i, log_likelihoods[i]))
        print("Difference of log likelihoods: {:.5f}\n".format(np.abs(log_likelihoods[i-1] - log_likelihoods[i])))
        
        if early_stopping:
            if np.abs(log_likelihoods[i-1] - log_likelihoods[i]) < epsilon:
                print("Converged after {} iterations.".format(i))
                break   
    
    return pi, mu, sigma, gamma, log_likelihoods
    
K = 4    
    
pi, mu, sigma, gamma, log_likelihoods = expectation_maximization(data, K)

# *** Plotting ***
log_likelihood_range = range(0, len(log_likelihoods))
plt.plot(log_likelihood_range, log_likelihoods, linewidth=2, linestyle="-")
plt.xlabel('Iteration')
plt.ylabel('Log Likelihood')
plt.savefig('figure_4_2_log_likelihoods.png')
plt.show()
    
colors = ['#148F77', '#3498DB', '#DC7633', '#85C1E9']

plt.figure()
# 'Predictions' are just based on the responsibilities/gammas
plt.scatter(data[:, 0], data[:, 1], c = [colors[x] for x in np.argmax(gamma, 1)], alpha = 0.75)
plt.xlabel('$x_1$')
plt.ylabel('$x_2$')
plt.savefig('figure_4_2_resp.png')
plt.show()    
\end{lstlisting}



\begin{figure}[H]
\center
\includegraphics[width=0.6\textwidth]{Images/figure_4_2_resp.png}
\caption{Component responsibilities of data points when k=4}
\label{Fig:responsibilities}
\end{figure}

\begin{figure}[H]
\center
\includegraphics[width=0.6\textwidth]{Images/figure_4_2_log_likelihoods.png}
\caption{Log Likelihoods after 100 iterations}
\label{Fig:log_likelihoods}
\end{figure}



\subsection*{4.3}

Set $K = 2$, initialize your random generator and run the EM-algorithm on the data. Describe what happens. Try different random initializations and compare results.\\
\textit{(Should converge within 50 steps to two clusters, accounting for $\pm 1/3$ resp. $2/3$ of the data)}. Plot the $x_1, x_2$ coordinates colored according to the most probable component. Compute the correlation coefficients 

\begin{eqnarray} \label{eq:8}
p_{12} = \frac{cov[x_1, x_2]}{\sqrt{var[x_1] var[x_2]}}
\end{eqnarray}

of each of the components (i.e., use their covariance matrices to compute variances and covariances in (\ref{eq:8}), see also (Bishop, eq. (2.93))). Does either class show the characteristic strong\footnote{According to Wikipedia, the correlation is none if $|p| < 0.1$, small if $0.1 < |p| < 0.3$, medium if $0.3 < |p| < 0.5$ and strong if $|p| > 0.5$} positive correlation for $\{ x_1, x_2\}$?\\


\textbf{Answer:}\\




The correlation for $x_1$ and $x_2$ are -0.325795621751765 for component  1 and -0.05387307557365256 for component 0. Therefore, there is no strong positive correlation.\\

The following Python code generated figure \ref{Fig:4_3_log_likelihoods} and \ref{Fig:4_3_responsibilities}.

\begin{lstlisting}[language=Python]
# Exercise 4.3

K = 2

pi, mu, sigma, gamma, log_likelihoods = expectation_maximization(data, K, early_stopping=True)

# *** Plotting ***
log_likelihood_range = range(0, len(log_likelihoods))
plt.plot(log_likelihood_range, log_likelihoods, linewidth=2, linestyle="-")
plt.xlabel('Iteration')
plt.ylabel('Log Likelihood')
plt.savefig('figure_4_3_log_likelihoods.png')
plt.show()
    
colors = ['#148F77', '#3498DB', '#DC7633', '#85C1E9']

plt.figure()
# 'Predictions' are just based on the responsibilities/gammas
plt.scatter(data[:, 0], data[:, 1], c = [colors[x] for x in np.argmax(gamma, 1)], alpha = 0.75)
plt.xlabel('$x_1$')
plt.ylabel('$x_2$')
plt.savefig('figure_4_3_resp.png')
plt.show()


correlation = [sigma[k][0, 1] / np.sqrt(sigma[k][0, 0] * sigma[k][1, 1]) for k in range(0, K)]
print("Correlation between x1 and x2: {}".format(correlation))
\end{lstlisting}

\begin{figure}[H]
\center
\includegraphics[width=0.6\textwidth]{Images/figure_4_3_resp.png}
\caption{Component responsibilities of data points when k=2}
\label{Fig:4_3_responsibilities}
\end{figure}

\begin{figure}[H]
\center
\includegraphics[width=0.6\textwidth]{Images/figure_4_3_log_likelihoods.png}
\caption{Log Likelihoods}
\label{Fig:4_3_log_likelihoods}
\end{figure}





\subsection*{4.4}

Increase the number of classes to $K = 3$ and rerun your algorithm on the data, again trying different random initializations. Plot the $x_1, x_2$ coordinates colored according to the most probable component and compute the correlation coefficients of each of the components. Check both your plot and your coefficients if one of the clusters now displays the strong positive $\{ x_1, x_2\}$ correlation we are looking for.\\
Increase to $K = 4$, do the same, and see if this improves your result (in terms of detection of the doping-clusters). Based on your findings, is the rumoured 1-in-5 estimate for users of X credible?\\


\textbf{Answer:}\\

Setting with $K = 3$\\

\begin{figure}[H]
\center
\includegraphics[width=0.6\textwidth]{Images/figure_4_4_a_resp.png}
\caption{Component responsibilities of data points when k=3}
\label{Fig:4_4_a_responsibilities}
\end{figure}

\begin{figure}[H]
\center
\includegraphics[width=0.6\textwidth]{Images/figure_4_4_a_log_likelihoods.png}
\caption{Log Likelihoods}
\label{Fig:4_4_a_log_likelihoods}
\end{figure}

The correlation between x1 and x2 for the different components are the following:

\begin{itemize}
	\item Component 0: 0.08087460101865655
	\item Component 1: -0.7230732922604372
	\item Component 2: -0.014620843890103857
\end{itemize}

At this point with $K = 3$ we cannot see any strong positive correlation.\\






Setting with $K=4$\\


\begin{figure}[H]
\center
\includegraphics[width=0.6\textwidth]{Images/figure_4_4_b_resp.png}
\caption{Component responsibilities of data points when k=4}
\label{Fig:4_4_b_responsibilities}
\end{figure}

\begin{figure}[H]
\center
\includegraphics[width=0.6\textwidth]{Images/figure_4_4_b_log_likelihoods.png}
\caption{Log Likelihoods}
\label{Fig:4_4_b_log_likelihoods}
\end{figure}

The correlation between x1 and x2 for the different components are the following:

\begin{itemize}
	\item Component 0: -0.894262700271773
	\item Component 1: 0.03626336720359386
	\item Component 2: 0.9151233789602066
	\item Component 3: -0.05253892018496478
\end{itemize}


When we increase the number of clusters/components to $K = 4$, we can see a strong positive correlation between x1 and x2 with component/cluster 2 with a coefficient of 0.9151233789602066.\\


The rumoured 1-in-5 estimate for users of X are credible and are actually a bit higher than 20\%, namely 21.2\%. The calculation is based on dividing the datapoints which belong to component 2 by the total number of datapoints. The corresponding Python code looks like this:

\begin{lstlisting}[language=Python]
percentage_of_drug_abuse = len(data[np.argmax(gamma, 1)==2,0])/N
print(percentage_of_drug_abuse)
# 0.212
\end{lstlisting}

\subsection*{4.5}

Having found the offending cluster in the data using the EM-algorithm, we are now presented with four samples $\{ A, B, C, D\}$, with values for $[x_1, x_2, x_3, x_4]$ given as 

\begin{align*}
A &= [11.85, 2.2, 0.5, 4.0]\\
B &= [11.95, 3.1, 0.0, 1.0]\\
C &= [12.00, 2.5, 0.0, 2.0]\\
D &= [12.00, 3.0, 1.0, 6.3]
\end{align*}

One of these is from a subject who took drug X, and one is from a subject who tried to tamper with the test by artificially altering one or more of the $x_i$ levels in his/her blood sample.\\

Identify which sample belongs to the suspected user and which one belongs to the 'fraud'.\\

\textbf{Answer:}\\

We can get the different likelihoods for the samples by executing the following Python code:

\begin{lstlisting}[language=Python]
# Exercise 4.5

samples = np.array([[11.85, 2.2, 0.5, 4.0],
                 [11.95, 3.1, 0.0, 1.0],
                 [12.00, 2.5, 0.0, 2.0],
                 [12.00, 3.0, 1.0, 6.3]])

likelihoods = np.zeros((len(samples), K))

for i in range(0, len(samples)):
    likelihood = [pi[k] * gaussian(samples[i], mu[k], sigma[k]) for k in range(0, K)]
    likelihood /= np.sum(likelihood)
    likelihoods[i] = likelihood

for i in range(len(likelihoods)):
    print("Likelihood for sample {}: {}".format(i, likelihoods[i]))
    
# Likelihood for sample 0: [4.12081596e-01 5.81451502e-01 4.98335178e-04 5.96856647e-03]
# Likelihood for sample 1: [9.17954224e-01 1.41990969e-05 2.87027450e-05 8.20028744e-02]
# Likelihood for sample 2: [6.71823809e-05 3.24443556e-05 9.99884498e-01 1.58757605e-05]
# Likelihood for sample 3: [1.28736925e-06 9.93178802e-01 4.62268466e-12 6.81991031e-03]
\end{lstlisting}

Based on the above likelihoods, sample C is probably a subject who took drug X because it belongs to component/cluster 2 with a confidence of 99.9\%. which has a strong positive correlation with x1 and x2.\\
Sample A probably belongs to the subject who tampered with with blood sample because the confidence is equally divided amongst component 0 and component 1. \\
Sample B and D probably belong to 'clean' athletes.


\end{document}
